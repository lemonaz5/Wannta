{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing IR Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup commonly used function and constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "import subprocess\n",
    "import time \n",
    "\n",
    "def save_object(obj, filename):\n",
    "    with open(filename, 'wb') as output:\n",
    "        pickle.dump(obj, output, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_object(filename):\n",
    "    r = {}\n",
    "    with open(filename, 'rb') as f:\n",
    "        r = pickle.load(f)\n",
    "    return r\n",
    "\n",
    "sentenceVectorFile = 'senVec.txt'\n",
    "sentenceDictFile = 'sen2vec.pkl'\n",
    "tokenizeFile = 'tokenized_out.txt'\n",
    "sentencesFile = 'sentences.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getSentenceVector(file = None, text = None, Format = True):\n",
    "    args = [\"/data2/fasttext/fasttext\", \"print-sentence-vectors\", \"/data2/cc.th.300.bin\"]\n",
    "    \n",
    "    if(text != None):\n",
    "        popen = subprocess.Popen(args,stdin=subprocess.PIPE, stdout=subprocess.PIPE)\n",
    "        output = popen.communicate(text.encode())[0]\n",
    "        popen.kill()\n",
    "        return np.array([line.split(' ')[:-1] for line in output.decode('utf8').split('\\n')[:-1]], dtype = np.float)\n",
    "    \n",
    "    elif(file != None):\n",
    "        f = open(file)\n",
    "        o = open(sentenceVectorFile, 'w')\n",
    "        popen = subprocess.Popen(args,stdin=f, stdout=o)   \n",
    "        popen.wait()\n",
    "#         output = popen.stdout.read()\n",
    "        f.close()\n",
    "        o.close()\n",
    "        popen.kill()\n",
    "#         if(Format):\n",
    "#             return np.array([line.split(' ')[:-1] for line in output.decode('utf8').split('\\n')[:-1]])\n",
    "#         else:\n",
    "#             return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pythainlp.tokenize import word_tokenize\n",
    "\n",
    "def sentenceTokenize(inputSentence):\n",
    "    # Tokenize\n",
    "    tokenized = word_tokenize(inputSentence)\n",
    "    newTokenize = []\n",
    "    for w in tokenized:\n",
    "        newTokenize += word_tokenize(w, engine='newmm')\n",
    "    return \" \".join(newTokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_memory():\n",
    "    sentences = []\n",
    "    with open(tokenizeFile, 'r') as fp:\n",
    "        for idx, line in enumerate(fp):\n",
    "            sentences.append(\" \".join(line.strip().split('|')))\n",
    "\n",
    "    with open(sentencesFile, 'w') as fp:\n",
    "        for idx, sen in enumerate(sentences):\n",
    "            if idx%2 != 0:\n",
    "                fp.write(\"{}\\n\".format(sen))\n",
    "  \n",
    "    getSentenceVector(file = sentencesFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.8 ms, sys: 8.31 ms, total: 21.1 ms\n",
      "Wall time: 10.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "prepare_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define prepare_model function\n",
    "By checking if file at 'sentenceDictFile' variable exist or not. If not it will create such file, otherwise it will load from file. The output of this function is sentenceDatabase which is a numpy array with dimension of length of all sentence in database by 300, each row is a sentence vector from precompiled fastText. Next is sen2vec is like sentenceDatabase but instead of index of number, the index is the sentence itself. And lastly idx2sen which connected the gap between the two previous mentioned variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_model():\n",
    "    sentencesTokenized = []\n",
    "    with open(tokenizeFile, 'r') as file:\n",
    "        for idx,line in enumerate(file.read().splitlines()):\n",
    "            if(idx%2 == 1):\n",
    "                sentencesTokenized.append(\"\".join(line.strip().split(\"|\")))\n",
    "    \n",
    "    if os.path.isfile(sentenceDictFile):\n",
    "        sen2vec = load_object(sentenceDictFile)\n",
    "        idx2sen = {}\n",
    "        for idx, sen in enumerate(sen2vec):\n",
    "            idx2sen[idx] = sen\n",
    "        \n",
    "        sentenceDatabase = np.zeros((len(sen2vec), 300))\n",
    "        for i in range(len(sen2vec)):\n",
    "            for j in range(300):\n",
    "                sentenceDatabase[i][j] = sen2vec[idx2sen[i]][j]\n",
    "        return sentenceDatabase, idx2sen, sen2vec\n",
    "    else:\n",
    "        sen2vec = {}\n",
    "        with open(sentenceVectorFile, 'r') as file:\n",
    "            for idx, line in enumerate(file):\n",
    "                vector = line.strip().split(' ')[-300:]\n",
    "                sentence = sentencesTokenized[idx]\n",
    "                sen2vec[sentence] = list(map(lambda x: float(x), vector))\n",
    "        save_object(sen2vec, sentenceDictFile)\n",
    "        idx2sen = {}\n",
    "        for idx, sen in enumerate(sen2vec):\n",
    "            idx2sen[idx] = sen\n",
    "        \n",
    "        sentenceDatabase = np.zeros((len(sen2vec), 300))\n",
    "        for i in range(len(sen2vec)):\n",
    "            for j in range(300):\n",
    "                sentenceDatabase[i][j] = sen2vec[idx2sen[i]][j]\n",
    "        return sentenceDatabase, idx2sen, sen2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 à¸ªà¸§à¸±à¸ªà¸”à¸µ\n",
      "1 à¸„à¸£à¸±à¸š\n"
     ]
    }
   ],
   "source": [
    "with open('/data2/test.txt') as f:\n",
    "    for i,e in enumerate(f.read().splitlines()):\n",
    "        print(i,e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define talkVec function\n",
    "This function compare inputSentenceVector (expected to be np array with dimension of (300, )) with rest of sentenceDatabase using consine similarity, and output the cloest sentence in database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def talkVec(sentenceDatabase, idx2sen, sen2vec, inputSentenceVector):\n",
    "    inputAb = np.linalg.norm(inputSentenceVector,ord=1)\n",
    "    output = sentenceDatabase.dot(inputSentenceVector)\n",
    "    for i in range(sentenceDatabase.shape[0]):\n",
    "        output[i] /= (np.linalg.norm(sentenceDatabase[i], ord=1))*inputAb\n",
    "    sumAll = np.sum(output)\n",
    "    output = output/sumAll\n",
    "    outIdx = np.argmax(output)\n",
    "    return idx2sen[outIdx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define main function\n",
    "As of right now. this is for testing only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparing...\n",
      "elaped 9.72899580001831\n",
      "> à¹à¸à¹‰à¸¡\n",
      "à¸«à¸²à¹€à¸£à¸·à¹ˆà¸­à¸‡à¸žà¸µà¹ˆà¹à¸à¹‰à¸¡ à¹€à¸”à¸°à¸¡à¸µà¸ªà¸§à¸¢à¸¢à¸¢\n",
      "elasped 9.233753442764282\n",
      "> à¹à¸à¹‰à¸¡ à¸—à¸³à¹„à¸£à¸­à¸¢à¸¹à¹ˆ\n",
      "à¸­à¸§à¸²à¸•à¸²à¸£à¹Œ555 à¹„à¸¡à¹ˆà¸£à¸¹à¹‰à¸ˆà¸°à¸—à¸³à¹„à¸‡ à¸„à¸·à¸­à¸«à¸™à¹‰à¸²repo à¸ˆà¸°à¸•à¹‰à¸­à¸‡à¸—à¸³à¹ƒà¸«à¹‰à¸”à¸¶à¸‡à¸¡à¸²à¸ˆà¸²à¸à¸”à¸²à¸•à¹‰à¸²à¹€à¸šà¸ª à¹‚à¸Šà¸§à¸”à¸µà¹„à¸§à¸ªà¹Œà¹€à¸›à¹‡à¸™à¸Šà¹ˆà¸­à¸‡à¹† (à¹€à¸›à¹‡à¸™rowà¸­à¹ˆà¸°)\n",
      "elasped 9.284680366516113\n",
      "> à¸—à¸³à¸­à¸°à¹„à¸µà¸­à¸¢à¸¹à¹ˆà¸­à¸°\n",
      "à¸Šà¸±à¹ˆà¸¢à¹à¸¥à¹‰à¸™ à¸¡à¸±à¸™à¸¡à¸µà¸ªà¸¢à¸²à¸¡à¸à¸°à¹€à¸‹à¸™à¹€à¸§à¸´à¸¥ à¸­à¹ˆà¸­à¸¡à¸µà¹€à¸—à¸­à¸¡à¸´à¸™à¸­à¸¥à¸”à¹‰à¸§à¸¢\n",
      "elasped 9.241783857345581\n",
      "> github\n",
      "/home/aroundy/Documents/wongnai_rating1.csv: No such file or directory ERROR: (gcloud.compute.scp) [/usr/bin/scp] exited with return code [1]. =____= à¸„à¸·à¸­à¸Šà¸±à¹‰à¸™à¸•à¹‰à¸­à¸‡à¸ªà¸£à¹‰à¸²à¸‡à¹„à¸Ÿà¸¥à¹Œà¸¡à¸²à¸£à¸­à¸‡à¸à¹ˆà¸­à¸™à¹€à¸£à¸­à¸°\n",
      "elasped 9.224480152130127\n",
      "> train model\n",
      "à¸­à¹ˆà¸­ à¸à¹‡à¸§à¹ˆà¸²à¸¡à¸±à¸™à¸žà¸±à¸‡ 555 à¸žà¸±à¸‡à¸­à¸¢à¸¹à¹ˆà¸”à¸µ output à¸¡à¸±à¸™à¸•à¹‰à¸­à¸‡à¹€à¸›à¹‡à¸™à¸­à¸°à¹„à¸£à¸­à¹ˆà¸° ValueError: Error when checking target: expected dense_4 to have shape (None, 5) but got array with shape (28000, 1)\n",
      "elasped 9.222252130508423\n",
      "> à¸žà¸™ à¸¡à¸µà¹€à¸£à¸µà¸¢à¸™à¸­à¸°à¹„à¸£à¸šà¹‰à¸²à¸‡\n",
      "Nsc à¸ˆà¸²à¸£à¸¢à¹Œà¹ƒà¸«à¹‰à¸£à¸·à¹‰à¸­uià¸¥à¸°à¸à¹‡à¸—à¸³à¹„à¸£à¹€à¸žà¸´à¹ˆà¸¡à¹€à¸¢à¸­à¸°à¸­à¸¢à¸¹à¹ˆ à¸„à¸·à¸­à¸ªà¹ˆà¸‡14 à¹à¸•à¹ˆà¸­à¸ˆà¸‚à¸­à¸”à¸¹à¸¨à¸¸à¸à¸£à¹Œà¸™à¸µà¹‰à¸à¹ˆà¸­à¸™\n",
      "elasped 9.277666330337524\n",
      "> à¹à¸à¹‰à¸¡ à¸žà¸™à¹€à¸£à¸µà¸¢à¸™à¸­à¸°à¹„à¸£à¸­à¸°\n",
      "à¸­à¸§à¸²à¸•à¸²à¸£à¹Œ555 à¹„à¸¡à¹ˆà¸£à¸¹à¹‰à¸ˆà¸°à¸—à¸³à¹„à¸‡ à¸„à¸·à¸­à¸«à¸™à¹‰à¸²repo à¸ˆà¸°à¸•à¹‰à¸­à¸‡à¸—à¸³à¹ƒà¸«à¹‰à¸”à¸¶à¸‡à¸¡à¸²à¸ˆà¸²à¸à¸”à¸²à¸•à¹‰à¸²à¹€à¸šà¸ª à¹‚à¸Šà¸§à¸”à¸µà¹„à¸§à¸ªà¹Œà¹€à¸›à¹‡à¸™à¸Šà¹ˆà¸­à¸‡à¹† (à¹€à¸›à¹‡à¸™rowà¸­à¹ˆà¸°)\n",
      "elasped 9.470296621322632\n",
      "> à¹„à¸‡\n",
      "à¸—à¸³à¹„à¸‡à¸”à¸µà¸­à¹ˆà¸° TT\n",
      "elasped 9.186060428619385\n",
      "> à¸—à¸³à¸à¸²à¸£à¸šà¹‰à¸²à¸™à¹€à¸ªà¸£à¹‡à¸ˆà¸¢à¸±à¸‡à¸­à¸°\n",
      "à¹€à¸ªà¸£à¹‡à¸ˆà¹à¸¥à¹‰à¸§ à¹à¸•à¹ˆà¸‡à¸²à¸™à¸¢à¸±à¸‡à¹„à¸¡à¹ˆà¹€à¸ªà¸£à¹‡à¸•ðŸ˜‚ à¸¡à¸µà¸à¸²à¸£à¸šà¹‰à¸²à¸™à¹€à¸—à¸„à¹‚à¸®à¸¡ à¸ªà¹ˆà¸‡à¸­à¸²à¸—à¸´à¸•à¸¢à¹Œ\n",
      "elasped 9.329479455947876\n",
      "> à¸à¸²à¸£à¸šà¹‰à¸²à¸™à¸§à¸´à¸Šà¸²à¹„à¸£à¸­à¸°\n",
      "à¹€à¸„à¹‰à¸²à¸„à¸·à¸­à¸§à¸‡à¹€à¸›à¸´à¸” 555 à¸§à¸±à¸™à¸™à¸µà¹‰à¹„à¸›à¹€à¸§à¸´à¸„à¸Šà¸­à¸›MSP à¸­à¹‰à¸§à¸™à¸žà¸µ à¹à¸ à¸žà¸£à¸¸à¹ˆà¸‡à¸™à¸µà¹‰à¹à¸à¹€à¸­à¸²à¹€à¸­à¸à¸ªà¸²à¸£à¹„à¸£à¹„à¸›à¸¡à¸°\n",
      "elasped 9.214555263519287\n",
      "> à¸žà¸£à¸¸à¹ˆà¸‡à¸™à¸µà¹‰à¹€à¸£à¸µà¸¢à¸™à¸­à¸°à¹„à¸£à¸šà¹‰à¸²à¸‡\n",
      "à¸‡à¸±à¹‰à¸™ à¸‚à¹‰à¸­à¸ªà¸¸à¸”à¸—à¹‰à¸²à¸¢à¹€à¸„à¹‰à¸²à¹ƒà¸«à¹‰à¸—à¸³à¸­à¸°à¹„à¸£ à¹€à¸—à¸µà¸¢à¸šà¸­à¸°à¹„à¸£à¸à¸±à¸šà¸­à¸°à¹„à¸£\n",
      "elasped 9.326479434967041\n",
      "> à¸—à¸³à¸à¸²à¸£à¸šà¹‰à¸²à¸™à¸§à¸´à¸Šà¸²à¹„à¸«à¸™à¹à¸¥à¹‰à¸§à¸šà¹‰à¸²à¸‡à¸­à¸°\n",
      "à¸­à¸µà¸à¸‚à¹‰à¸­à¸¡à¸±à¸™à¸¡à¸µà¹ƒà¸™à¸ªà¹„à¸¥à¸”à¹Œà¸™à¸´ à¸à¹‡à¸ªà¸±à¹ˆà¸‡à¹à¸à¹‰à¹à¸¥à¹‰à¸§à¸­à¹ˆà¸° à¹à¸•à¹ˆà¹à¸à¹‰à¸—à¸µà¸™à¸¶à¸‡à¸à¸§à¹ˆà¸²à¸ˆà¸°à¹„à¸”à¹‰à¸‡à¸²à¸™ à¹€à¸›à¹‡à¸™à¸§à¸±à¸™ à¸—à¸±à¹‰à¸‡à¸—à¸µà¹ˆà¹à¸à¹‰à¹à¸„à¹ˆà¸™à¸´à¸”à¹€à¸”à¸µà¸¢à¸§ à¸„à¸·à¸­à¸¡à¸µà¸„à¸™à¸¢à¸±à¸‡à¹„à¸¡à¹ˆà¸ªà¹ˆà¸‡à¹à¸à¹‰à¸•à¸±à¹‰à¸‡à¹à¸•à¹ˆà¸ªà¸±à¹ˆà¸‡à¹„à¸›à¹€à¸¡à¸·à¹ˆà¸­à¸­à¸±à¸‡à¸„à¸²à¸£à¹€à¸¥à¸¢ à¸—à¸±à¹‰à¸‡à¸—à¸µà¹ˆà¸›à¸²à¸à¸¡à¸±à¸™à¸šà¸­à¸à¹à¸à¹‰à¹à¸›à¸›à¹€à¸”à¸µà¸¢à¸§à¹† à¸­à¸¡à¸ à¸—à¸³à¹„à¸¡à¸¡à¸¡à¸¡ à¸›à¸§à¸”à¸•à¸²à¸¥à¸°à¸™à¸°à¹à¸‡à¸‡à¸‡\n",
      "elasped 9.274222135543823\n",
      "> à¹€à¸­à¸à¸žà¸¥\n",
      "à¸¥à¸°à¸„à¸™à¸­à¸·à¹ˆà¸™à¹†à¸¥à¸°à¸°à¸° à¹à¸à¹„à¸›à¸«à¸²à¸­à¸ˆà¹€à¸­à¸à¸›à¹ˆà¸²à¸§\n",
      "elasped 9.249753475189209\n",
      "> à¹„à¸›à¸«à¸² à¸­à¸ˆà¹€à¸­à¸\n",
      "à¸¥à¸°à¸„à¸™à¸­à¸·à¹ˆà¸™à¹†à¸¥à¸°à¸°à¸° à¹à¸à¹„à¸›à¸«à¸²à¸­à¸ˆà¹€à¸­à¸à¸›à¹ˆà¸²à¸§\n",
      "elasped 9.318080425262451\n",
      "> à¹„à¸›à¹†\n",
      "à¸—à¸³à¹† à¹„à¸›à¸¥à¸°à¹„à¸›à¹€à¸œà¸¥à¸­à¸¥à¸š à¹à¸›à¸¥à¸‡unk à¸‚à¸­à¸‡ test\n",
      "elasped 9.262150764465332\n",
      "> à¸žà¸µà¸£à¸žà¸¥\n",
      "à¹‚à¸›à¸£à¹€à¸ˆà¸„à¸­à¸ˆà¸žà¸µà¸£à¸žà¸¥à¹„à¸‡555 à¹„à¸›à¸„à¸¸à¸¢\n",
      "elasped 9.501237392425537\n",
      "> à¸­à¸²à¸ˆà¸²à¸£à¸¢à¹Œà¸žà¸µà¸£à¸žà¸¥\n",
      "à¹‚à¸›à¸£à¹€à¸ˆà¸„à¸­à¸ˆà¸žà¸µà¸£à¸žà¸¥à¹„à¸‡555 à¹„à¸›à¸„à¸¸à¸¢\n",
      "elasped 9.234578847885132\n",
      "> à¸‡à¸‡ à¸žà¸µà¸£à¸žà¸¥\n",
      "à¹‚à¸›à¸£à¹€à¸ˆà¸„à¸­à¸ˆà¸žà¸µà¸£à¸žà¸¥à¹„à¸‡555 à¹„à¸›à¸„à¸¸à¸¢\n",
      "elasped 9.459232091903687\n",
      "> nlp\n",
      "à¸Šà¹ˆà¸²à¸‡à¹€à¸–à¸­à¸° à¹€à¸­à¸²à¹€à¸›à¹‡à¸™à¸§à¹ˆà¸²à¸‚à¸¶à¹‰à¸™à¸à¸±à¸šà¸—à¸¸à¸à¸„à¸µà¸¢à¹Œà¸žà¸£à¹‰à¸­à¸¡à¸à¸±à¸™555\n",
      "elasped 9.604577541351318\n",
      "> à¸à¸²à¸£à¸šà¹‰à¸²à¸™ nlp\n",
      "nlp\n",
      "elasped 9.222375631332397\n",
      "> à¹€à¸à¸£à¸” nlp\n",
      "nlp\n",
      "elasped 9.448594808578491\n",
      "> à¹€à¸à¸£à¸”\n",
      "à¸‚à¸µà¹‰à¹‚à¸¡à¹‰ à¹€à¸—à¸­à¸„à¸™à¹€à¸”à¸µà¸¢à¸§à¸¢à¸±à¸‡à¹€à¸à¹ˆà¸‡à¸à¸§à¹ˆà¸²à¸—à¸µà¸¡à¸Šà¸±à¹‰à¸™à¹€à¸¥à¸¢à¸¡à¸±à¹‰à¸‡\n",
      "elasped 9.238069772720337\n",
      "> à¸šà¸¸à¸£à¸´à¸™\n",
      "à¸™à¸µà¹ˆà¸šà¸¸à¸£à¸´à¸™ à¸§à¹ˆà¸²à¸¥à¸°\n",
      "elasped 9.376887559890747\n",
      "> à¹à¸Ÿà¸£à¹Œ\n",
      "à¹à¸Ÿà¸£à¹Œà¸„à¸™à¹à¸—à¸£à¸›à¸›à¸µà¹‰ à¹à¸—à¸£à¸›à¸›à¸µà¹‰à¹à¸Ÿ à¸ˆà¸°à¸ªà¹ˆà¸‡à¸šà¸¹à¹‰à¹„à¸›à¸à¸±à¸”\n",
      "elasped 9.217392206192017\n",
      "> à¸šà¸¹à¸šà¸¹à¹‰\n",
      "à¹„à¸›à¸«à¸²à¸šà¸¹à¹‰à¸”à¸µà¸à¹ˆà¸²\n",
      "elasped 9.253528356552124\n",
      "> à¸šà¸¹à¹‰à¸­à¸¢à¸¹à¹ˆà¹„à¸«à¸™\n",
      "à¸­à¸¢à¸¹à¹ˆà¸à¸±à¸šà¸šà¸¹à¹‰\n",
      "elasped 9.198312759399414\n",
      "> à¸šà¸¹à¹‰à¸„à¸·à¸­à¹„à¸£\n",
      "à¸­à¸§à¸²à¸•à¸²à¸£à¹Œ555 à¹„à¸¡à¹ˆà¸£à¸¹à¹‰à¸ˆà¸°à¸—à¸³à¹„à¸‡ à¸„à¸·à¸­à¸«à¸™à¹‰à¸²repo à¸ˆà¸°à¸•à¹‰à¸­à¸‡à¸—à¸³à¹ƒà¸«à¹‰à¸”à¸¶à¸‡à¸¡à¸²à¸ˆà¸²à¸à¸”à¸²à¸•à¹‰à¸²à¹€à¸šà¸ª à¹‚à¸Šà¸§à¸”à¸µà¹„à¸§à¸ªà¹Œà¹€à¸›à¹‡à¸™à¸Šà¹ˆà¸­à¸‡à¹† (à¹€à¸›à¹‡à¸™rowà¸­à¹ˆà¸°)\n",
      "elasped 9.22165298461914\n",
      "> à¸„à¸§à¸²à¸¡à¸ªà¸¸à¸‚à¸—à¸µà¹ˆà¸«à¹ˆà¸²à¸‡à¸«à¸²à¸¢ à¹„à¸¡à¹ˆà¸£à¸¹à¹‰à¸§à¹ˆà¸²à¸­à¸¢à¸¹à¹ˆà¹„à¸«à¸™à¹à¸„à¹ˆà¹€à¸žà¸µà¸¢à¸‡à¹€à¸›à¸´à¸”à¹ƒà¸ˆ à¸ˆà¸°à¸£à¸¹à¹‰à¹„à¸¡à¹ˆà¹„à¸à¸¥à¸ˆà¸²à¸à¸•à¸£à¸‡à¸™à¸µà¹‰\n",
      "à¸„à¸·à¸­à¹€à¸„à¹‰à¸²à¸à¹‡à¸¢à¸±à¸‡à¹„à¸¡à¹ˆà¸£à¸¹à¹‰à¸£à¸²à¸¢à¸¥à¸°à¹€à¸­à¸µà¸¢à¸”à¹€à¸—à¹ˆà¸²à¹„à¸«à¸£à¹ˆ à¹€à¸”à¸°à¸£à¸¹à¹‰à¸§à¸±à¸™à¹„à¸›à¹à¸™à¹ˆà¸Šà¸±à¸”à¸¥à¸°à¸ˆà¸°à¸šà¸­à¸à¸­à¸µà¸à¸—à¸µ à¹à¸•à¹ˆà¸‚à¸²à¹„à¸›à¹à¸¡à¹ˆà¸™à¹ˆà¸²à¸ˆà¸°à¹„à¸›à¹€à¸—à¸µà¹ˆà¸¢à¸§à¸à¸±à¸™à¸à¹ˆà¸­à¸™à¸ªà¸²à¸¡à¸ªà¸µà¹ˆà¸§à¸±à¸™\n",
      "elasped 9.217488050460815\n",
      "> à¸—à¸µà¹ˆà¸—à¸µà¹ˆà¸—à¹‰à¸­à¸‡à¸Ÿà¹‰à¸² à¸„à¸·à¸™à¸„à¸§à¸²à¸¡à¸ªà¸”à¹ƒà¸ª à¸—à¸µà¹ˆà¸—à¸µà¹ˆà¸”à¸­à¸à¹„à¸¡à¹‰ à¸ˆà¸°à¸•à¹ˆà¸­à¹€à¸•à¸´à¸¡à¹‚à¸¥à¸à¹ƒà¸«à¹‰à¹€à¸•à¹‡à¸¡à¹ƒà¸š à¹à¸„à¹ˆà¹€à¸£à¸²à¹„à¸”à¹‰à¸¡à¸­à¸‡à¸”à¸¹ à¸à¹‡à¸ˆà¸°à¸£à¸¹à¹‰à¸„à¸§à¸²à¸¡à¸ªà¸¸à¸‚à¸­à¸¢à¸¹à¹ˆà¹„à¸¡à¹ˆà¹„à¸à¸¥\n",
      "à¸«à¸¡à¸²à¸¢à¸„à¸§à¸²à¸¡à¸§à¹ˆà¸²à¸‡à¹Šà¸²à¸¢555 à¸Šà¸±à¹‰à¸™à¹„à¸¡à¹ˆà¸­à¹‰à¸§à¸™à¸™à¸™à¸°à¸° à¸«à¸™à¸±à¸à¹„à¸¡à¹ˆà¹€à¸à¸´à¸™46à¹à¸™à¹ˆà¸™à¸­à¸™ à¸§à¸´à¸”à¸µà¹‚à¸­à¸ªà¸•à¸£à¸µà¸¡à¸¡à¸´à¹ˆà¸‡ à¸¡à¸µà¹ƒà¸™à¸šà¸—2à¹à¸•à¹ˆà¹€à¸«à¸¡à¸·à¸­à¸™à¸ˆà¸°à¸­à¸±à¸žà¹€à¸”à¸—à¸¡à¸²à¹ƒà¸«à¸¡à¹ˆ à¸®à¸¶à¹ˆà¸¡à¸¡à¸¡à¸¡ à¹ƒà¸™à¹€à¸­à¸à¸ªà¸²à¸£à¸„à¸³à¸ªà¸­à¸™à¸à¹‡à¹„à¸¡à¹ˆà¸¡à¸µ\n",
      "elasped 9.212650299072266\n",
      "> à¸›à¸µà¸à¸™à¹‰à¸­à¸¢à¹† à¹€à¸”à¸´à¸™à¸—à¸²à¸‡à¸•à¸²à¸¡à¸«à¸²à¸„à¸§à¸²à¸¡à¸à¸±à¸™à¸­à¸±à¸™à¸¢à¸´à¹ˆà¸‡à¹ƒà¸«à¸à¹ˆ à¸¡à¸­à¸‡à¸­à¸­à¸à¹„à¸›à¸ªà¸¸à¸”à¸Ÿà¹‰à¸²à¹„à¸à¸¥ à¸•à¸²à¸¡à¸­à¸‡à¸«à¸²à¸§à¹ˆà¸²à¸›à¸¥à¸²à¸¢à¸—à¸²à¸‡à¸­à¸¢à¸¹à¹ˆà¸—à¸µà¹ˆà¹ƒà¸”\n",
      "à¹„à¸¡à¹ˆà¸£à¸¹à¹‰555 à¸¡à¸²à¹€à¸‰à¸¥à¸¢à¸­à¸±à¸™à¹€à¸à¹ˆà¸²à¸à¹‡à¹„à¸”à¹‰ 555 à¹€à¸”à¸´à¸™à¸¡à¸²à¸‹à¸­à¸¢à¸«à¸¥à¸±à¸‡à¸§à¸±à¸”à¸«à¸±à¸§à¸¥à¸³à¹‚à¸žà¸‡à¸­à¹ˆà¸° à¹€à¸­à¸²à¸‡à¸µà¹‰ à¹€à¸‚à¹‰à¸²à¸‹à¸­à¸¢à¹‚à¸£à¸‡à¹€à¸£à¸µà¸¢à¸™à¸§à¸±à¸”à¸«à¸±à¸§à¸¥à¸³à¹‚à¸žà¸‡à¸¥à¸°à¹€à¸”à¸´à¸™à¸•à¸£à¸‡à¸¡à¸²à¹€à¸£à¸·à¹ˆà¸­à¸¢à¹† à¸¡à¸µà¸ªà¸µà¹ˆà¹à¸¢à¸à¸à¹‡à¸‚à¹‰à¸²à¸¡à¸–à¸™à¸™à¸¡à¸²à¸‹à¸­à¸¢à¸ªà¸±à¸™à¸•à¸´à¸ à¸²à¸ž1 à¹€à¸”à¸´à¸™à¸•à¸£à¸‡à¸¡à¸²à¸­à¸µà¸à¸™à¸´à¸”à¸™à¸¶à¸‡ à¸›à¸£à¸°à¸•à¸¹à¸£à¸±à¹‰à¸§à¸ªà¸µà¸‚à¸²à¸§à¸”à¹‰à¸²à¸™à¸‹à¹‰à¸²à¸¢ à¸à¸”à¸­à¸­à¸”à¸”à¹‰à¸§à¸¢ à¸šà¹‰à¸²à¸™à¹€à¸¥à¸‚à¸—à¸µà¹ˆ128\n",
      "elasped 9.20193099975586\n",
      "> à¸—à¸µà¹ˆà¸¡à¸µà¸„à¸§à¸²à¸¡à¸£à¸±à¸\n",
      "à¸«à¸¡à¸²à¸¢à¸–à¸¶à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸—à¸µà¹ˆà¸«à¸™à¹‰à¸²à¸”à¸µà¹„à¸§à¸ªà¹Œà¹ƒà¸Šà¹ˆà¸¡à¸° à¸à¹‡à¸¡à¸µà¸—à¸¸à¸à¸­à¸¢à¹ˆà¸²à¸‡à¸—à¸µà¹ˆà¹€à¸£à¸²à¸ªà¸£à¹‰à¸²à¸‡à¸­à¹ˆà¸° à¸žà¸§à¸à¹€à¸ˆà¸™à¸„à¸µà¸¢à¹Œà¹ƒà¸«à¸¡à¹ˆà¸”à¹‰à¸§à¸¢à¸›à¹ˆà¸° à¹ƒà¸Šà¹ˆà¹†à¸¥à¸°à¸à¹‡à¸•à¹‰à¸­à¸‡à¸¡à¸µà¸žà¸§à¸à¸›à¸¸à¹ˆà¸¡à¸à¸”\n",
      "elasped 9.269872188568115\n",
      "> à¸—à¸¸à¸à¸­à¸¢à¹ˆà¸²à¸‡à¸—à¸µà¹ˆà¹€à¸£à¸²à¸ªà¸£à¹‰à¸²à¸‡\n",
      "à¸­à¸±à¸™à¸™à¸±à¹‰à¸™à¹€à¸‚à¹‰à¸²à¹ƒà¸ˆ à¹à¸•à¹ˆà¹€à¸„à¹‰à¸²à¸à¸³à¸¥à¸±à¸‡à¸žà¸¹à¸”à¸–à¸¶à¸‡à¸­à¸±à¸™à¸—à¸µà¹ˆà¹„à¸¡à¹ˆà¹„à¸”à¹‰à¹€à¸›à¹‡à¸™à¸•à¸²à¸£à¸²à¸‡à¸­à¸¢à¹ˆà¸²à¸‡à¸žà¸§à¸à¸šà¸´à¸¥à¹„à¸‡ðŸ˜‚\n",
      "elasped 9.290881395339966\n",
      "> à¸¡à¸µà¹à¸•à¹ˆà¹€à¸£à¸·à¹ˆà¸­à¸‡à¹€à¸£à¸µà¸¢à¸™à¸™à¸™à¸™à¸™\n",
      "à¸ªà¸­à¸™à¸™à¸™à¸™à¸™ à¹€à¸—à¸žà¸ªà¸´ à¸•à¸±à¹‰à¸‡400 à¸„à¸§à¸´à¸‹2à¹„à¸¡à¹ˆà¸¡à¸µà¹€à¸‚à¸µà¸¢à¸™à¹‚à¸„à¹Šà¸”à¸¥à¸°à¹ƒà¸Šà¹ˆà¸¡à¸°\n",
      "elasped 9.389853477478027\n",
      "> à¸—à¸³à¹„à¸¡à¹€à¸à¹ˆà¸‡à¸ˆà¸±à¸‡à¹€à¸¥à¸¢\n",
      "à¹€à¸à¹ˆà¸‡à¸à¸±à¸™à¸ˆà¸±à¸‡ à¸—à¸³à¹„à¸¡à¹à¸§à¹ˆà¸™à¹€à¸›à¹‹à¸­à¹„à¸¡à¹ˆà¹„à¸”à¹‰à¸­à¸°à¹„à¸£à¹€à¸¥à¸¢\n",
      "elasped 9.218095541000366\n",
      "> à¹à¸§à¹ˆà¸™à¹€à¸›à¹‹à¸­\n",
      "à¹€à¸à¸´à¸”à¹€à¸›à¹‡à¸™à¹à¸§à¹ˆà¸™à¹€à¸›à¹‹à¸­à¸Šà¹ˆà¸²à¸‡à¹€à¸«à¸™à¸·à¹ˆà¸­à¸¢à¸¢à¸²à¸555\n",
      "elasped 9.226080656051636\n",
      "> à¸—à¸³à¹„à¸¡ à¹à¸§à¹ˆà¸™à¹€à¸›à¹‹à¸­\n",
      "à¹€à¸à¹ˆà¸‡à¸à¸±à¸™à¸ˆà¸±à¸‡ à¸—à¸³à¹„à¸¡à¹à¸§à¹ˆà¸™à¹€à¸›à¹‹à¸­à¹„à¸¡à¹ˆà¹„à¸”à¹‰à¸­à¸°à¹„à¸£à¹€à¸¥à¸¢\n",
      "elasped 9.291785955429077\n",
      "> à¸„à¸™à¹„à¸¡à¹ˆà¸ˆà¸³à¹€à¸›à¹‡à¸™à¸à¹‡à¸•à¹‰à¸­à¸‡à¹€à¸”à¸´à¸™à¸ˆà¸²à¸à¹„à¸›\n",
      "à¹‚à¸›à¸£à¹€à¸ˆà¸„à¸à¹‡à¹‚à¸Šà¸§à¹Œà¸Šà¸·à¹ˆà¸­ à¸œà¸¹à¸à¸à¸±à¸šà¸”à¸µà¹„à¸§à¸ªà¹Œà¸­à¸°à¹„à¸£à¸­à¸¢à¸¹à¹ˆà¸šà¹‰à¸²à¸‡ à¸¥à¸°à¸„à¸§à¸²à¸¡à¹€à¸ªà¸µà¹ˆà¸¢à¸‡à¹€à¸›à¹‡à¸™à¹„à¸‡ (à¸™à¹‰à¸­à¸¢-à¸›à¸²à¸™à¸à¸¥à¸²à¸‡-à¸¡à¸²à¸ à¹à¸ªà¸”à¸‡à¹€à¸›à¹‡à¸™à¸ªà¸µà¸à¹‡à¹„à¸”à¹‰) à¸¥à¸°à¸Šà¹ˆà¸­à¸‡à¸ªà¸¸à¸”à¸—à¹‰à¸²à¸¢à¸à¹‡à¸„à¸¥à¸´à¸à¹€à¸¥à¸·à¸­à¸à¹€à¸«à¸¡à¸·à¸­à¸™à¸”à¸µà¹„à¸§à¸ªà¹Œ à¹€à¸„à¹‰à¸²à¹€à¸‚à¹‰à¸²à¹„à¸›à¸„à¸“à¸°à¸§à¸±à¸™à¸—à¸µà¹ˆ4à¸­à¹ˆà¸° à¹„à¸›à¸à¸§à¸¨ à¹„à¸›à¸¡à¸°555\n",
      "elasped 9.212727069854736\n",
      "> sa\n",
      ";eU)n+AG9I<+ohc7Nk&X\n",
      "elasped 9.262389421463013\n",
      "> SA\n",
      "à¸§à¸±à¸™à¸™à¸µà¹‰à¸¡à¸µà¸™à¸±à¸”SA\n",
      "elasped 9.285059690475464\n",
      "> à¹€à¸›à¹‡à¸”à¸™à¹‰à¸­à¸¢\n",
      "à¸«à¸¥à¸±à¸š555 à¹„à¸¡à¹ˆà¹à¸žà¸‡à¸–à¹‰à¸²à¹€à¸›à¹‡à¸™à¸«à¹‰à¸­à¸‡à¹à¸–à¸§à¹€à¸¥à¹‡à¸à¹† à¹à¸–à¸§à¸‹à¸­à¸¢à¸«à¸¥à¸±à¸‡à¸§à¸±à¸”à¸«à¸±à¸§à¸¥à¸³à¹‚à¸žà¸‡\n",
      "elasped 9.334018230438232\n",
      "> à¸«à¸²à¸à¸§à¹ˆà¸²à¹€à¸˜à¸­à¸œà¹ˆà¸²à¸™à¸¡à¸²à¹„à¸”à¹‰à¸¢à¸´à¸™ à¹€à¸žà¸¥à¸‡à¸™à¸µà¹‰ à¸„à¸²à¸”à¸§à¹ˆà¸²à¹€à¸˜à¸­à¸à¹‡à¸„à¸‡à¸£à¸¹à¹‰à¸”à¸µ à¸§à¹ˆà¸²à¹€à¸›à¹‡à¸™à¸‰à¸±à¸™ à¸¡à¸­à¸šà¹ƒà¸«à¹‰à¹€à¸˜à¸­à¸„à¸™à¹€à¸”à¸µà¸¢à¸§ à¸­à¸²à¸ˆà¹„à¸¡à¹ˆà¹€à¸à¸µà¹ˆà¸¢à¸§à¹à¸•à¹ˆà¸ªà¸³à¸„à¸±à¸ à¹€à¸žà¸£à¸²à¸°à¸‰à¸±à¸™ à¸™à¸±à¹‰à¸™à¸­à¸¢à¸²à¸à¹ƒà¸«à¹‰à¹€à¸˜à¸­à¹„à¸”à¹‰à¸Ÿà¸±à¸‡\n",
      "à¸•à¸­à¸™à¹à¸£à¸à¸„à¸´à¸”à¸§à¹ˆà¸²à¹„à¸¡à¹ˆà¸•à¸±à¸” à¹à¸•à¹ˆà¸„à¸´à¸”à¹„à¸›à¸„à¸´à¸”à¸¡à¸² à¸¡à¸±à¸™à¹€à¸›à¹‡à¸™ unk à¹à¸¥à¹‰à¸§à¹à¸›à¸¥à¸§à¹ˆà¸²à¹„à¸¡à¹ˆà¸¡à¸µà¹ƒà¸™ dict à¸à¹‡à¸•à¹‰à¸­à¸‡à¸•à¸±à¸” à¹à¸•à¹ˆà¸•à¸­à¸™à¸™à¸µà¹‰à¸à¸³à¸¥à¸±à¸‡à¸„à¸´à¸”à¸§à¹ˆà¸²à¸‚à¸à¸£à¸±à¸™à¹ƒà¸«à¸¡à¹ˆ à¸•à¹ˆà¸­à¹ƒà¸«à¹‰à¹„à¸¡à¹ˆà¸•à¸±à¸”à¸à¹‡à¸™à¹ˆà¸²à¸ˆà¸°à¹„à¸¡à¹ˆà¸ªà¹ˆà¸‡à¸œà¸¥à¹„à¸£ à¸£à¸¶à¹€à¸›à¸¥à¹ˆà¸²à¸«à¸§à¹ˆà¸² 555\n",
      "elasped 9.215057611465454\n",
      "> à¸—à¸µà¹ˆà¹€à¸à¹‰à¸²à¹à¸•à¹ˆà¹‚à¸—à¸£ à¹‚à¸—à¸£à¹„à¸›à¸«à¸²à¹€à¸˜à¸­ à¹€à¸£à¸·à¹ˆà¸­à¸¢à¹€à¸›à¸·à¹ˆà¸­à¸¢ à¹€à¸«à¸™à¸·à¹ˆà¸­à¸¢à¸à¹‡à¸¢à¸­à¸¡ à¸à¹‡à¹ƒà¸ˆà¸Šà¸­à¸šà¹€à¸˜à¸­ à¹„à¸¡à¹ˆà¹€à¸šà¸² à¹à¸•à¹ˆà¹ƒà¸ˆà¹€à¸ˆà¹‰à¸²à¸à¸£à¸£à¸¡à¹„à¸¡à¹ˆà¸£à¸¹à¹‰à¹€à¸¥à¸¢ à¸§à¹ˆà¸²à¹€à¸˜à¸­à¹„à¸¡à¹ˆà¸Šà¸­à¸šà¹€à¸£à¸² à¹„à¸¡à¹ˆà¸ªà¸™à¹à¸¥à¸°à¹„à¸¡à¹ˆà¹€à¸­à¸² à¹„à¸¡à¹ˆà¸­à¸¢à¸²à¸à¸„à¸¸à¸¢\n",
      "à¹à¸à¸£à¹Š à¹à¸¢à¹ˆà¹à¸«à¸¥à¹ˆà¸§ à¹€à¸¡à¸·à¹ˆà¸­à¸à¸µà¹‰à¸žà¸¢à¸²à¸¢à¸²à¸¡à¸—à¸³tokenize à¹€à¸«à¸¡à¸·à¸­à¸™à¸ˆà¸°à¸¢à¸±à¸”à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹€à¸¢à¸­à¸°à¹„à¸› à¹€à¸„à¸£à¸·à¹ˆà¸­à¸‡à¸„à¹‰à¸²à¸‡ à¹€à¸¡à¸¡à¹€à¸•à¹‡à¸¡ à¸—à¸³à¹„à¸£à¹„à¸¡à¹ˆà¹„à¸”à¹‰à¹€à¸¥à¸­ à¸à¹‡à¹€à¸¥à¸¢à¸ªà¸±à¹ˆà¸‡à¸›à¸´à¸”vmà¹€à¸¥à¸¢ à¸•à¸­à¸™à¸™à¸µà¹‰à¸¥à¸¸à¹‰à¸™à¸­à¸¢à¸¹à¹ˆà¸§à¹ˆà¸²à¸ˆà¸°à¹€à¸›à¸´à¸”à¸‚à¸¶à¹‰à¸™à¸¡à¸±à¹‰à¸¢ à¸–à¸–à¸–à¸–à¸–à¸–à¸– à¸Šà¸µà¸§à¸´à¸•à¸¡à¸µà¹à¸•à¹ˆà¸­à¸¸à¸›à¸ªà¹Šà¸£à¸£à¸£à¸£à¸„\n",
      "elasped 9.166561841964722\n",
      "> token word à¸¢à¸±à¸‡à¹„à¸‡à¸­à¸°\n",
      "à¸­à¹ˆà¸­ à¸à¹‡à¸§à¹ˆà¸²à¸¡à¸±à¸™à¸žà¸±à¸‡ 555 à¸žà¸±à¸‡à¸­à¸¢à¸¹à¹ˆà¸”à¸µ output à¸¡à¸±à¸™à¸•à¹‰à¸­à¸‡à¹€à¸›à¹‡à¸™à¸­à¸°à¹„à¸£à¸­à¹ˆà¸° ValueError: Error when checking target: expected dense_4 to have shape (None, 5) but got array with shape (28000, 1)\n",
      "elasped 9.238011598587036\n",
      "> \n",
      "CPU times: user 4.92 s, sys: 10.3 s, total: 15.2 s\n",
      "Wall time: 28min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def main():\n",
    "    print('preparing...')\n",
    "    start = time.time()\n",
    "    prepare_memory()\n",
    "    sentenceDatabase, idx2sen, sen2vec = prepare_model()\n",
    "    print('elaped', time.time() - start)\n",
    "    while(True):\n",
    "        print('>', end= ' ')\n",
    "#     text = \"à¸—à¸³à¸‡à¸²à¸™ database à¸¢à¸±à¸‡\"\n",
    "        text = input()\n",
    "        if(text == ''):\n",
    "            break\n",
    "        start = time.time()\n",
    "        text = sentenceTokenize(text)\n",
    "        sent_vec = getSentenceVector(text = text)[0]\n",
    "        print(talkVec(sentenceDatabase, idx2sen, sen2vec, sent_vec))\n",
    "        print(\"elasped\", time.time() - start)\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing another IR Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentenceHumanVectorFile = 'senHumanVec.txt'\n",
    "sentenceHumanDictFile = 'senHuman2vec.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_model_human():\n",
    "    if os.path.isfile(sentenceHumanDictFile):\n",
    "        senHuman2vec = load_object(sentenceHumanDictFile)\n",
    "        idx2senHuman = {}\n",
    "        for idx, sen in enumerate(senHuman2vec):\n",
    "            idx2senHuman[idx] = sen\n",
    "        \n",
    "        sentenceHumanDatabase = np.zeros((len(senHuman2vec), 300))\n",
    "        for i in range(len(senHuman2vec)):\n",
    "            for j in range(300):\n",
    "                sentenceHumanDatabase[i][j] = senHuman2vec[idx2senHuman[i]][j]\n",
    "        return sentenceHumanDatabase, idx2senHuman, senHuman2vec\n",
    "    else:\n",
    "        senHuman2vec = {}\n",
    "        with open(sentenceHumanVectorFile, 'r') as file:\n",
    "            for idx, line in enumerate(file):\n",
    "                sentence = line.strip().split(' ')\n",
    "                vector = sentence[-300:]\n",
    "                sentence = \"\".join(sentence[:len(sentence) - 300])\n",
    "                senHuman2vec[sentence] = list(map(lambda x: float(x), vector))\n",
    "        save_object(senHuman2vec, sentenceHumanDictFile)\n",
    "        \n",
    "        idx2senHuman = {}\n",
    "        for idx, sen in enumerate(senHuman2vec):\n",
    "            idx2senHuman[idx] = sen\n",
    "        \n",
    "        sentenceHumanDatabase = np.zeros((len(senHuman2vec), 300))\n",
    "        for i in range(len(senHuman2vec)):\n",
    "            for j in range(300):\n",
    "                sentenceHumanDatabase[i][j] = senHuman2vec[idx2senHuman[i]][j]\n",
    "        return sentenceHumanDatabase, idx2senHuman, senHuman2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def talkHmanVec(sentenceHumanDatabase, idx2senHuman, senHuman2vec, idx2sen, inputSentenceVector):\n",
    "    inputAb = np.linalg.norm(inputSentenceVector,ord=1)\n",
    "    output = sentenceHumanDatabase.dot(inputSentenceVector)\n",
    "    for i in range(sentenceHumanDatabase.shape[0]):\n",
    "        output[i] /= (np.linalg.norm(sentenceHumanDatabase[i], ord=1))*inputAb\n",
    "    sumAll = np.sum(output)\n",
    "    output = output/sumAll\n",
    "    outIdx = np.argmax(output)\n",
    "    return idx2sen[outIdx+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'senHumanVec.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-aa6dd633f96c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-16-aa6dd633f96c>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0msentenceDatabase\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx2sen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msen2vec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprepare_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0msentenceHumanDatabase\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx2senHuman\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msenHuman2vec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprepare_model_human\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mtestIdx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midx2sen\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtestIdx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-963b4d70b898>\u001b[0m in \u001b[0;36mprepare_model_human\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0msenHuman2vec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentenceHumanVectorFile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m                 \u001b[0msentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'senHumanVec.txt'"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    sentenceDatabase, idx2sen, sen2vec = prepare_model()\n",
    "    sentenceHumanDatabase, idx2senHuman, senHuman2vec = prepare_model_human()\n",
    "    testIdx = 300\n",
    "    print(idx2sen[testIdx])\n",
    "    print(talkHmanVec(sentenceDatabase, idx2sen, sen2vec, idx2sen, np.array(sen2vec[idx2sen[testIdx]])))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
