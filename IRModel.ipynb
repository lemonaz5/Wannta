{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing IR Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup commonly used function and constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "import subprocess\n",
    "import time \n",
    "\n",
    "def save_object(obj, filename):\n",
    "    with open(filename, 'wb') as output:\n",
    "        pickle.dump(obj, output, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_object(filename):\n",
    "    r = {}\n",
    "    with open(filename, 'rb') as f:\n",
    "        r = pickle.load(f)\n",
    "    return r\n",
    "\n",
    "# sentenceVectorFile = 'senVec-3k.txt'\n",
    "# sentenceDictFile = 'sen2vec-3k.pkl'\n",
    "# tokenizeFile = 'tokenized_out-3k.txt'\n",
    "# sentencesFile = 'sentences-3k.txt'\n",
    "\n",
    "sentenceVectorFile = 'senVec-30k.txt'\n",
    "sentenceDictFile = 'sen2vec-30k.pkl'\n",
    "tokenizeFile = 'tokenized_out-30k.txt'\n",
    "sentencesFile = 'sentences-30k.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getSentenceVector(file = None, text = None, Format = True):\n",
    "    args = [\"/data2/fasttext/fasttext\", \"print-sentence-vectors\", \"/data2/cc.th.300.bin\"]\n",
    "    \n",
    "    if(text != None):\n",
    "        popen = subprocess.Popen(args,stdin=subprocess.PIPE, stdout=subprocess.PIPE)\n",
    "        output = popen.communicate(text.encode())[0]\n",
    "        popen.kill()\n",
    "        return np.array([line.split(' ')[:-1] for line in output.decode('utf8').split('\\n')[:-1]], dtype = np.float)\n",
    "    \n",
    "    elif(file != None):\n",
    "        f = open(file)\n",
    "        o = open(sentenceVectorFile, 'w')\n",
    "        popen = subprocess.Popen(args,stdin=f, stdout=o)   \n",
    "        popen.wait()\n",
    "#         output = popen.stdout.read()\n",
    "        f.close()\n",
    "        o.close()\n",
    "        popen.kill()\n",
    "#         if(Format):\n",
    "#             return np.array([line.split(' ')[:-1] for line in output.decode('utf8').split('\\n')[:-1]])\n",
    "#         else:\n",
    "#             return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pythainlp.tokenize import word_tokenize\n",
    "\n",
    "def sentenceTokenize(inputSentence):\n",
    "    # Tokenize\n",
    "    tokenized = word_tokenize(inputSentence)\n",
    "    newTokenize = []\n",
    "    for w in tokenized:\n",
    "        newTokenize += word_tokenize(w, engine='newmm')\n",
    "    return \" \".join(newTokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_memory():\n",
    "    sentences = []\n",
    "    with open(tokenizeFile, 'r') as fp:\n",
    "        for idx, line in enumerate(fp):\n",
    "            sentences.append(\" \".join(line.strip().split('|')))\n",
    "\n",
    "    with open(sentencesFile, 'w') as fp:\n",
    "        for idx, sen in enumerate(sentences):\n",
    "            if idx%2 != 0:\n",
    "                fp.write(\"{}\\n\".format(sen))\n",
    "  \n",
    "    getSentenceVector(file = sentencesFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define prepare_model function\n",
    "By checking if file at 'sentenceDictFile' variable exist or not. If not it will create such file, otherwise it will load from file. The output of this function is sentenceDatabase which is a numpy array with dimension of length of all sentence in database by 300, each row is a sentence vector from precompiled fastText. Next is sen2vec is like sentenceDatabase but instead of index of number, the index is the sentence itself. And lastly idx2sen which connected the gap between the two previous mentioned variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_model():\n",
    "    sentencesTokenized = []\n",
    "    with open(tokenizeFile, 'r') as file:\n",
    "        for idx,line in enumerate(file.read().splitlines()):\n",
    "            if(idx%2 == 1):\n",
    "                sentencesTokenized.append(\"\".join(line.strip().split(\"|\")))\n",
    "    \n",
    "    if os.path.isfile(sentenceDictFile):\n",
    "        sen2vec = load_object(sentenceDictFile)\n",
    "        idx2sen = {}\n",
    "        for idx, sen in enumerate(sen2vec):\n",
    "            idx2sen[idx] = sen\n",
    "        \n",
    "        sentenceDatabase = np.zeros((len(sen2vec), 300))\n",
    "        for i in range(len(sen2vec)):\n",
    "            for j in range(300):\n",
    "                sentenceDatabase[i][j] = sen2vec[idx2sen[i]][j]\n",
    "        return sentenceDatabase, idx2sen, sen2vec\n",
    "    else:\n",
    "        sen2vec = {}\n",
    "        with open(sentenceVectorFile, 'r') as file:\n",
    "            for idx, line in enumerate(file):\n",
    "                vector = line.strip().split(' ')[-300:]\n",
    "                sentence = sentencesTokenized[idx]\n",
    "                sen2vec[sentence] = list(map(lambda x: float(x), vector))\n",
    "        save_object(sen2vec, sentenceDictFile)\n",
    "        idx2sen = {}\n",
    "        for idx, sen in enumerate(sen2vec):\n",
    "            idx2sen[idx] = sen\n",
    "        \n",
    "        sentenceDatabase = np.zeros((len(sen2vec), 300))\n",
    "        for i in range(len(sen2vec)):\n",
    "            for j in range(300):\n",
    "                sentenceDatabase[i][j] = sen2vec[idx2sen[i]][j]\n",
    "        return sentenceDatabase, idx2sen, sen2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define talkVec function\n",
    "This function compare inputSentenceVector (expected to be np array with dimension of (300, )) with rest of sentenceDatabase using consine similarity, and output the cloest sentence in database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def talkVec(sentenceDatabase, idx2sen, sen2vec, inputSentenceVector):\n",
    "    inputAb = np.linalg.norm(inputSentenceVector)\n",
    "    output = sentenceDatabase.dot(inputSentenceVector)\n",
    "    for i in range(sentenceDatabase.shape[0]):\n",
    "        if ((np.linalg.norm(sentenceDatabase[i]))*inputAb) == 0:\n",
    "            output[i] = 0\n",
    "        else:\n",
    "            output[i] /= ((np.linalg.norm(sentenceDatabase[i]))*inputAb)\n",
    "#     sumAll = np.sum(output)\n",
    "#     output = output/sumAll\n",
    "    outIdx = np.argmax(output)\n",
    "    print(output[outIdx])\n",
    "    return idx2sen[outIdx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define main function\n",
    "As of right now. this is for testing only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 1e+03 ns, total: 5 µs\n",
      "Wall time: 8.34 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def main():\n",
    "    print('preparing...')\n",
    "    start = time.time()\n",
    "    prepare_memory()\n",
    "    sentenceDatabase, idx2sen, sen2vec = prepare_model()\n",
    "    print('elaped', time.time() - start)\n",
    "    while(True):\n",
    "        print('>', end= ' ')\n",
    "#     text = \"ทำงาน database ยัง\"\n",
    "        text = input()\n",
    "        if(text == ''):\n",
    "            break\n",
    "        start = time.time()\n",
    "        text = sentenceTokenize(text)\n",
    "        sent_vec = getSentenceVector(text = text)[0]\n",
    "        print(talkVec(sentenceDatabase, idx2sen, sen2vec, sent_vec))\n",
    "        print(\"elasped\", time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparing...\n",
      "elaped 34.47522163391113\n",
      "> ตามมม\n",
      "0.6670774360665618\n",
      "ตัดอยู่ เห้อมม\n",
      "elasped 19.4144446849823\n",
      "> ตัดไรจ้ะ\n",
      "0.7674505563896165\n",
      "ไรจ๊ะ\n",
      "elasped 19.434170246124268\n",
      "> นาน\n",
      "0.8143211527431622\n",
      "ปลงนานแล้ว\n",
      "elasped 16.295188426971436\n",
      "> ต๋าาา\n",
      "0.7184825658610016\n",
      "เอ้า\n",
      "elasped 25.25016951560974\n",
      "> \n",
      "CPU times: user 3.44 s, sys: 1.23 s, total: 4.67 s\n",
      "Wall time: 32min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparing...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-85-c466246d8e77>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'time'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'main()'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/ekapolc/.env/lib/python3.5/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[1;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[0;32m   2291\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2292\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2293\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2294\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2295\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[1;32m/home/ekapolc/.env/lib/python3.5/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(f, *a, **k)\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[1;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m         \u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ekapolc/.env/lib/python3.5/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[0;32m   1161\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'eval'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1162\u001b[0m             \u001b[0mst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1163\u001b[1;33m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1164\u001b[0m             \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1165\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-51-725b7522cf9f>\u001b[0m in \u001b[0;36mprepare_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msen2vec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m                 \u001b[0msentenceDatabase\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msen2vec\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx2sen\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0msentenceDatabase\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx2sen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msen2vec\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing another IR Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentenceHumanVectorFile = 'senHumanVec.txt'\n",
    "sentenceHumanDictFile = 'senHuman2vec.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_model_human():\n",
    "    if os.path.isfile(sentenceHumanDictFile):\n",
    "        senHuman2vec = load_object(sentenceHumanDictFile)\n",
    "        idx2senHuman = {}\n",
    "        for idx, sen in enumerate(senHuman2vec):\n",
    "            idx2senHuman[idx] = sen\n",
    "        \n",
    "        sentenceHumanDatabase = np.zeros((len(senHuman2vec), 300))\n",
    "        for i in range(len(senHuman2vec)):\n",
    "            for j in range(300):\n",
    "         sentenceHumanDatabase[i][j] = senHuman2vec[idx2senHuman[i]][j]\n",
    "        return sentenceHumanDatabase, idx2senHuman, senHuman2vec\n",
    "    else:\n",
    "        senHuman2vec = {}\n",
    "        with open(sentenceHumanVectorFile, 'r') as file:\n",
    "            for idx, line in enumerate(file):\n",
    "                sentence = line.strip().split(' ')\n",
    "                vector = sentence[-300:]\n",
    "                sentence = \"\".join(sentence[:len(sentence) - 300])\n",
    "                senHuman2vec[sentence] = list(map(lambda x: float(x), vector))\n",
    "        save_object(senHuman2vec, sentenceHumanDictFile)\n",
    "        \n",
    "        idx2senHuman = {}\n",
    "        for idx, sen in enumerate(senHuman2vec):\n",
    "            idx2senHuman[idx] = sen\n",
    "        \n",
    "        sentenceHumanDatabase = np.zeros((len(senHuman2vec), 300))\n",
    "        for i in range(len(senHuman2vec)):\n",
    "            for j in range(300):\n",
    "                sentenceHumanDatabase[i][j] = senHuman2vec[idx2senHuman[i]][j]\n",
    "        return sentenceHumanDatabase, idx2senHuman, senHuman2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def talkHmanVec(sentenceHumanDatabase, idx2senHuman, senHuman2vec, idx2sen, inputSentenceVector):\n",
    "    inputAb = np.linalg.norm(inputSentenceVector,ord=1)\n",
    "    output = sentenceHumanDatabase.dot(inputSentenceVector)\n",
    "    for i in range(sentenceHumanDatabase.shape[0]):\n",
    "        output[i] /= (np.linalg.norm(sentenceHumanDatabase[i], ord=1))*inputAb\n",
    "    sumAll = np.sum(output)\n",
    "    output = output/sumAll\n",
    "    outIdx = np.argmax(output)\n",
    "    return idx2sen[outIdx+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'senHumanVec.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-aa6dd633f96c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-16-aa6dd633f96c>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0msentenceDatabase\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx2sen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msen2vec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprepare_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0msentenceHumanDatabase\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx2senHuman\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msenHuman2vec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprepare_model_human\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mtestIdx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midx2sen\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtestIdx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-963b4d70b898>\u001b[0m in \u001b[0;36mprepare_model_human\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0msenHuman2vec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentenceHumanVectorFile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m                 \u001b[0msentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'senHumanVec.txt'"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    sentenceDatabase, idx2sen, sen2vec = prepare_model()\n",
    "    sentenceHumanDatabase, idx2senHuman, senHuman2vec = prepare_model_human()\n",
    "    testIdx = 300\n",
    "    print(idx2sen[testIdx])\n",
    "    print(talkHmanVec(sentenceDatabase, idx2sen, sen2vec, idx2sen, np.array(sen2vec[idx2sen[testIdx]])))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
