{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing IR Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup commonly used function and constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "import subprocess\n",
    "import time \n",
    "\n",
    "def save_object(obj, filename):\n",
    "    with open(filename, 'wb') as output:\n",
    "        pickle.dump(obj, output, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_object(filename):\n",
    "    r = {}\n",
    "    with open(filename, 'rb') as f:\n",
    "        r = pickle.load(f)\n",
    "    return r\n",
    "\n",
    "sentenceVectorFile = 'senVec.txt'\n",
    "sentenceDictFile = 'sen2vec.pkl'\n",
    "tokenizeFile = 'tokenized_out.txt'\n",
    "sentencesFile = 'sentences.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getSentenceVector(file = None, text = None, Format = True):\n",
    "    args = [\"/data2/fasttext/fasttext\", \"print-sentence-vectors\", \"/data2/cc.th.300.bin\"]\n",
    "    \n",
    "    if(text != None):\n",
    "        popen = subprocess.Popen(args,stdin=subprocess.PIPE, stdout=subprocess.PIPE)\n",
    "        output = popen.communicate(text.encode())[0]\n",
    "        popen.kill()\n",
    "        return np.array([line.split(' ')[:-1] for line in output.decode('utf8').split('\\n')[:-1]], dtype = np.float)\n",
    "    \n",
    "    elif(file != None):\n",
    "        f = open(file)\n",
    "        o = open(sentenceVectorFile, 'w')\n",
    "        popen = subprocess.Popen(args,stdin=f, stdout=o)   \n",
    "        popen.wait()\n",
    "#         output = popen.stdout.read()\n",
    "        f.close()\n",
    "        o.close()\n",
    "        popen.kill()\n",
    "#         if(Format):\n",
    "#             return np.array([line.split(' ')[:-1] for line in output.decode('utf8').split('\\n')[:-1]])\n",
    "#         else:\n",
    "#             return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pythainlp.tokenize import word_tokenize\n",
    "\n",
    "def sentenceTokenize(inputSentence):\n",
    "    # Tokenize\n",
    "    tokenized = word_tokenize(inputSentence)\n",
    "    newTokenize = []\n",
    "    for w in tokenized:\n",
    "        newTokenize += word_tokenize(w, engine='newmm')\n",
    "    return \" \".join(newTokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_memory():\n",
    "    sentences = []\n",
    "    with open(tokenizeFile, 'r') as fp:\n",
    "        for idx, line in enumerate(fp):\n",
    "            sentences.append(\" \".join(line.strip().split('|')))\n",
    "\n",
    "    with open(sentencesFile, 'w') as fp:\n",
    "        for idx, sen in enumerate(sentences):\n",
    "            if idx%2 != 0:\n",
    "                fp.write(\"{}\\n\".format(sen))\n",
    "  \n",
    "    getSentenceVector(file = sentencesFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.8 ms, sys: 8.31 ms, total: 21.1 ms\n",
      "Wall time: 10.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "prepare_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define prepare_model function\n",
    "By checking if file at 'sentenceDictFile' variable exist or not. If not it will create such file, otherwise it will load from file. The output of this function is sentenceDatabase which is a numpy array with dimension of length of all sentence in database by 300, each row is a sentence vector from precompiled fastText. Next is sen2vec is like sentenceDatabase but instead of index of number, the index is the sentence itself. And lastly idx2sen which connected the gap between the two previous mentioned variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_model():\n",
    "    sentencesTokenized = []\n",
    "    with open(tokenizeFile, 'r') as file:\n",
    "        for idx,line in enumerate(file.read().splitlines()):\n",
    "            if(idx%2 == 1):\n",
    "                sentencesTokenized.append(\"\".join(line.strip().split(\"|\")))\n",
    "    \n",
    "    if os.path.isfile(sentenceDictFile):\n",
    "        sen2vec = load_object(sentenceDictFile)\n",
    "        idx2sen = {}\n",
    "        for idx, sen in enumerate(sen2vec):\n",
    "            idx2sen[idx] = sen\n",
    "        \n",
    "        sentenceDatabase = np.zeros((len(sen2vec), 300))\n",
    "        for i in range(len(sen2vec)):\n",
    "            for j in range(300):\n",
    "                sentenceDatabase[i][j] = sen2vec[idx2sen[i]][j]\n",
    "        return sentenceDatabase, idx2sen, sen2vec\n",
    "    else:\n",
    "        sen2vec = {}\n",
    "        with open(sentenceVectorFile, 'r') as file:\n",
    "            for idx, line in enumerate(file):\n",
    "                vector = line.strip().split(' ')[-300:]\n",
    "                sentence = sentencesTokenized[idx]\n",
    "                sen2vec[sentence] = list(map(lambda x: float(x), vector))\n",
    "        save_object(sen2vec, sentenceDictFile)\n",
    "        idx2sen = {}\n",
    "        for idx, sen in enumerate(sen2vec):\n",
    "            idx2sen[idx] = sen\n",
    "        \n",
    "        sentenceDatabase = np.zeros((len(sen2vec), 300))\n",
    "        for i in range(len(sen2vec)):\n",
    "            for j in range(300):\n",
    "                sentenceDatabase[i][j] = sen2vec[idx2sen[i]][j]\n",
    "        return sentenceDatabase, idx2sen, sen2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 สวัสดี\n",
      "1 ครับ\n"
     ]
    }
   ],
   "source": [
    "with open('/data2/test.txt') as f:\n",
    "    for i,e in enumerate(f.read().splitlines()):\n",
    "        print(i,e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define talkVec function\n",
    "This function compare inputSentenceVector (expected to be np array with dimension of (300, )) with rest of sentenceDatabase using consine similarity, and output the cloest sentence in database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def talkVec(sentenceDatabase, idx2sen, sen2vec, inputSentenceVector):\n",
    "    inputAb = np.linalg.norm(inputSentenceVector,ord=1)\n",
    "    output = sentenceDatabase.dot(inputSentenceVector)\n",
    "    for i in range(sentenceDatabase.shape[0]):\n",
    "        output[i] /= (np.linalg.norm(sentenceDatabase[i], ord=1))*inputAb\n",
    "    sumAll = np.sum(output)\n",
    "    output = output/sumAll\n",
    "    outIdx = np.argmax(output)\n",
    "    return idx2sen[outIdx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define main function\n",
    "As of right now. this is for testing only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparing...\n",
      "elaped 9.72899580001831\n",
      "> แก้ม\n",
      "หาเรื่องพี่แก้ม เดะมีสวยยย\n",
      "elasped 9.233753442764282\n",
      "> แก้ม ทำไรอยู่\n",
      "อวาตาร์555 ไม่รู้จะทำไง คือหน้าrepo จะต้องทำให้ดึงมาจากดาต้าเบส โชวดีไวส์เป็นช่องๆ (เป็นrowอ่ะ)\n",
      "elasped 9.284680366516113\n",
      "> ทำอะไีอยู่อะ\n",
      "ชั่ยแล้น มันมีสยามกะเซนเวิล อ่อมีเทอมินอลด้วย\n",
      "elasped 9.241783857345581\n",
      "> github\n",
      "/home/aroundy/Documents/wongnai_rating1.csv: No such file or directory ERROR: (gcloud.compute.scp) [/usr/bin/scp] exited with return code [1]. =____= คือชั้นต้องสร้างไฟล์มารองก่อนเรอะ\n",
      "elasped 9.224480152130127\n",
      "> train model\n",
      "อ่อ ก็ว่ามันพัง 555 พังอยู่ดี output มันต้องเป็นอะไรอ่ะ ValueError: Error when checking target: expected dense_4 to have shape (None, 5) but got array with shape (28000, 1)\n",
      "elasped 9.222252130508423\n",
      "> พน มีเรียนอะไรบ้าง\n",
      "Nsc จารย์ให้รื้อuiละก็ทำไรเพิ่มเยอะอยู่ คือส่ง14 แต่อจขอดูศุกร์นี้ก่อน\n",
      "elasped 9.277666330337524\n",
      "> แก้ม พนเรียนอะไรอะ\n",
      "อวาตาร์555 ไม่รู้จะทำไง คือหน้าrepo จะต้องทำให้ดึงมาจากดาต้าเบส โชวดีไวส์เป็นช่องๆ (เป็นrowอ่ะ)\n",
      "elasped 9.470296621322632\n",
      "> ไง\n",
      "ทำไงดีอ่ะ TT\n",
      "elasped 9.186060428619385\n",
      "> ทำการบ้านเสร็จยังอะ\n",
      "เสร็จแล้ว แต่งานยังไม่เสร็ต😂 มีการบ้านเทคโฮม ส่งอาทิตย์\n",
      "elasped 9.329479455947876\n",
      "> การบ้านวิชาไรอะ\n",
      "เค้าคือวงเปิด 555 วันนี้ไปเวิคชอปMSP อ้วนพี แก พรุ่งนี้แกเอาเอกสารไรไปมะ\n",
      "elasped 9.214555263519287\n",
      "> พรุ่งนี้เรียนอะไรบ้าง\n",
      "งั้น ข้อสุดท้ายเค้าให้ทำอะไร เทียบอะไรกับอะไร\n",
      "elasped 9.326479434967041\n",
      "> ทำการบ้านวิชาไหนแล้วบ้างอะ\n",
      "อีกข้อมันมีในสไลด์นิ ก็สั่งแก้แล้วอ่ะ แต่แก้ทีนึงกว่าจะได้งาน เป็นวัน ทั้งที่แก้แค่นิดเดียว คือมีคนยังไม่ส่งแก้ตั้งแต่สั่งไปเมื่ออังคารเลย ทั้งที่ปากมันบอกแก้แปปเดียวๆ อมก ทำไมมมม ปวดตาละนะแงงง\n",
      "elasped 9.274222135543823\n",
      "> เอกพล\n",
      "ละคนอื่นๆละะะ แกไปหาอจเอกป่าว\n",
      "elasped 9.249753475189209\n",
      "> ไปหา อจเอก\n",
      "ละคนอื่นๆละะะ แกไปหาอจเอกป่าว\n",
      "elasped 9.318080425262451\n",
      "> ไปๆ\n",
      "ทำๆ ไปละไปเผลอลบ แปลงunk ของ test\n",
      "elasped 9.262150764465332\n",
      "> พีรพล\n",
      "โปรเจคอจพีรพลไง555 ไปคุย\n",
      "elasped 9.501237392425537\n",
      "> อาจารย์พีรพล\n",
      "โปรเจคอจพีรพลไง555 ไปคุย\n",
      "elasped 9.234578847885132\n",
      "> งง พีรพล\n",
      "โปรเจคอจพีรพลไง555 ไปคุย\n",
      "elasped 9.459232091903687\n",
      "> nlp\n",
      "ช่างเถอะ เอาเป็นว่าขึ้นกับทุกคีย์พร้อมกัน555\n",
      "elasped 9.604577541351318\n",
      "> การบ้าน nlp\n",
      "nlp\n",
      "elasped 9.222375631332397\n",
      "> เกรด nlp\n",
      "nlp\n",
      "elasped 9.448594808578491\n",
      "> เกรด\n",
      "ขี้โม้ เทอคนเดียวยังเก่งกว่าทีมชั้นเลยมั้ง\n",
      "elasped 9.238069772720337\n",
      "> บุริน\n",
      "นี่บุริน ว่าละ\n",
      "elasped 9.376887559890747\n",
      "> แฟร์\n",
      "แฟร์คนแทรปปี้ แทรปปี้แฟ จะส่งบู้ไปกัด\n",
      "elasped 9.217392206192017\n",
      "> บูบู้\n",
      "ไปหาบู้ดีก่า\n",
      "elasped 9.253528356552124\n",
      "> บู้อยู่ไหน\n",
      "อยู่กับบู้\n",
      "elasped 9.198312759399414\n",
      "> บู้คือไร\n",
      "อวาตาร์555 ไม่รู้จะทำไง คือหน้าrepo จะต้องทำให้ดึงมาจากดาต้าเบส โชวดีไวส์เป็นช่องๆ (เป็นrowอ่ะ)\n",
      "elasped 9.22165298461914\n",
      "> ความสุขที่ห่างหาย ไม่รู้ว่าอยู่ไหนแค่เพียงเปิดใจ จะรู้ไม่ไกลจากตรงนี้\n",
      "คือเค้าก็ยังไม่รู้รายละเอียดเท่าไหร่ เดะรู้วันไปแน่ชัดละจะบอกอีกที แต่ขาไปแม่น่าจะไปเที่ยวกันก่อนสามสี่วัน\n",
      "elasped 9.217488050460815\n",
      "> ที่ที่ท้องฟ้า คืนความสดใส ที่ที่ดอกไม้ จะต่อเติมโลกให้เต็มใบ แค่เราได้มองดู ก็จะรู้ความสุขอยู่ไม่ไกล\n",
      "หมายความว่าง๊าย555 ชั้นไม่อ้วนนนะะ หนักไม่เกิน46แน่นอน วิดีโอสตรีมมิ่ง มีในบท2แต่เหมือนจะอัพเดทมาใหม่ ฮึ่มมมม ในเอกสารคำสอนก็ไม่มี\n",
      "elasped 9.212650299072266\n",
      "> ปีกน้อยๆ เดินทางตามหาความฝันอันยิ่งใหญ่ มองออกไปสุดฟ้าไกล ตามองหาว่าปลายทางอยู่ที่ใด\n",
      "ไม่รู้555 มาเฉลยอันเก่าก็ได้ 555 เดินมาซอยหลังวัดหัวลำโพงอ่ะ เอางี้ เข้าซอยโรงเรียนวัดหัวลำโพงละเดินตรงมาเรื่อยๆ มีสี่แยกก็ข้ามถนนมาซอยสันติภาพ1 เดินตรงมาอีกนิดนึง ประตูรั้วสีขาวด้านซ้าย กดออดด้วย บ้านเลขที่128\n",
      "elasped 9.20193099975586\n",
      "> ที่มีความรัก\n",
      "หมายถึงข้อมูลที่หน้าดีไวส์ใช่มะ ก็มีทุกอย่างที่เราสร้างอ่ะ พวกเจนคีย์ใหม่ด้วยป่ะ ใช่ๆละก็ต้องมีพวกปุ่มกด\n",
      "elasped 9.269872188568115\n",
      "> ทุกอย่างที่เราสร้าง\n",
      "อันนั้นเข้าใจ แต่เค้ากำลังพูดถึงอันที่ไม่ได้เป็นตารางอย่างพวกบิลไง😂\n",
      "elasped 9.290881395339966\n",
      "> มีแต่เรื่องเรียนนนนน\n",
      "สอนนนนน เทพสิ ตั้ง400 ควิซ2ไม่มีเขียนโค๊ดละใช่มะ\n",
      "elasped 9.389853477478027\n",
      "> ทำไมเก่งจังเลย\n",
      "เก่งกันจัง ทำไมแว่นเป๋อไม่ได้อะไรเลย\n",
      "elasped 9.218095541000366\n",
      "> แว่นเป๋อ\n",
      "เกิดเป็นแว่นเป๋อช่างเหนื่อยยาก555\n",
      "elasped 9.226080656051636\n",
      "> ทำไม แว่นเป๋อ\n",
      "เก่งกันจัง ทำไมแว่นเป๋อไม่ได้อะไรเลย\n",
      "elasped 9.291785955429077\n",
      "> คนไม่จำเป็นก็ต้องเดินจากไป\n",
      "โปรเจคก็โชว์ชื่อ ผูกกับดีไวส์อะไรอยู่บ้าง ละความเสี่ยงเป็นไง (น้อย-ปานกลาง-มาก แสดงเป็นสีก็ได้) ละช่องสุดท้ายก็คลิกเลือกเหมือนดีไวส์ เค้าเข้าไปคณะวันที่4อ่ะ ไปกวศ ไปมะ555\n",
      "elasped 9.212727069854736\n",
      "> sa\n",
      ";eU)n+AG9I<+ohc7Nk&X\n",
      "elasped 9.262389421463013\n",
      "> SA\n",
      "วันนี้มีนัดSA\n",
      "elasped 9.285059690475464\n",
      "> เป็ดน้อย\n",
      "หลับ555 ไม่แพงถ้าเป็นห้องแถวเล็กๆ แถวซอยหลังวัดหัวลำโพง\n",
      "elasped 9.334018230438232\n",
      "> หากว่าเธอผ่านมาได้ยิน เพลงนี้ คาดว่าเธอก็คงรู้ดี ว่าเป็นฉัน มอบให้เธอคนเดียว อาจไม่เกี่ยวแต่สำคัญ เพราะฉัน นั้นอยากให้เธอได้ฟัง\n",
      "ตอนแรกคิดว่าไม่ตัด แต่คิดไปคิดมา มันเป็น unk แล้วแปลว่าไม่มีใน dict ก็ต้องตัด แต่ตอนนี้กำลังคิดว่าขกรันใหม่ ต่อให้ไม่ตัดก็น่าจะไม่ส่งผลไร รึเปล่าหว่า 555\n",
      "elasped 9.215057611465454\n",
      "> ที่เฝ้าแต่โทร โทรไปหาเธอ เรื่อยเปื่อย เหนื่อยก็ยอม ก็ใจชอบเธอ ไม่เบา แต่ใจเจ้ากรรมไม่รู้เลย ว่าเธอไม่ชอบเรา ไม่สนและไม่เอา ไม่อยากคุย\n",
      "แกร๊ แย่แหล่ว เมื่อกี้พยายามทำtokenize เหมือนจะยัดข้อมูลเยอะไป เครื่องค้าง เมมเต็ม ทำไรไม่ได้เลอ ก็เลยสั่งปิดvmเลย ตอนนี้ลุ้นอยู่ว่าจะเปิดขึ้นมั้ย ถถถถถถถ ชีวิตมีแต่อุปส๊รรรรค\n",
      "elasped 9.166561841964722\n",
      "> token word ยังไงอะ\n",
      "อ่อ ก็ว่ามันพัง 555 พังอยู่ดี output มันต้องเป็นอะไรอ่ะ ValueError: Error when checking target: expected dense_4 to have shape (None, 5) but got array with shape (28000, 1)\n",
      "elasped 9.238011598587036\n",
      "> \n",
      "CPU times: user 4.92 s, sys: 10.3 s, total: 15.2 s\n",
      "Wall time: 28min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def main():\n",
    "    print('preparing...')\n",
    "    start = time.time()\n",
    "    prepare_memory()\n",
    "    sentenceDatabase, idx2sen, sen2vec = prepare_model()\n",
    "    print('elaped', time.time() - start)\n",
    "    while(True):\n",
    "        print('>', end= ' ')\n",
    "#     text = \"ทำงาน database ยัง\"\n",
    "        text = input()\n",
    "        if(text == ''):\n",
    "            break\n",
    "        start = time.time()\n",
    "        text = sentenceTokenize(text)\n",
    "        sent_vec = getSentenceVector(text = text)[0]\n",
    "        print(talkVec(sentenceDatabase, idx2sen, sen2vec, sent_vec))\n",
    "        print(\"elasped\", time.time() - start)\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing another IR Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentenceHumanVectorFile = 'senHumanVec.txt'\n",
    "sentenceHumanDictFile = 'senHuman2vec.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_model_human():\n",
    "    if os.path.isfile(sentenceHumanDictFile):\n",
    "        senHuman2vec = load_object(sentenceHumanDictFile)\n",
    "        idx2senHuman = {}\n",
    "        for idx, sen in enumerate(senHuman2vec):\n",
    "            idx2senHuman[idx] = sen\n",
    "        \n",
    "        sentenceHumanDatabase = np.zeros((len(senHuman2vec), 300))\n",
    "        for i in range(len(senHuman2vec)):\n",
    "            for j in range(300):\n",
    "                sentenceHumanDatabase[i][j] = senHuman2vec[idx2senHuman[i]][j]\n",
    "        return sentenceHumanDatabase, idx2senHuman, senHuman2vec\n",
    "    else:\n",
    "        senHuman2vec = {}\n",
    "        with open(sentenceHumanVectorFile, 'r') as file:\n",
    "            for idx, line in enumerate(file):\n",
    "                sentence = line.strip().split(' ')\n",
    "                vector = sentence[-300:]\n",
    "                sentence = \"\".join(sentence[:len(sentence) - 300])\n",
    "                senHuman2vec[sentence] = list(map(lambda x: float(x), vector))\n",
    "        save_object(senHuman2vec, sentenceHumanDictFile)\n",
    "        \n",
    "        idx2senHuman = {}\n",
    "        for idx, sen in enumerate(senHuman2vec):\n",
    "            idx2senHuman[idx] = sen\n",
    "        \n",
    "        sentenceHumanDatabase = np.zeros((len(senHuman2vec), 300))\n",
    "        for i in range(len(senHuman2vec)):\n",
    "            for j in range(300):\n",
    "                sentenceHumanDatabase[i][j] = senHuman2vec[idx2senHuman[i]][j]\n",
    "        return sentenceHumanDatabase, idx2senHuman, senHuman2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def talkHmanVec(sentenceHumanDatabase, idx2senHuman, senHuman2vec, idx2sen, inputSentenceVector):\n",
    "    inputAb = np.linalg.norm(inputSentenceVector,ord=1)\n",
    "    output = sentenceHumanDatabase.dot(inputSentenceVector)\n",
    "    for i in range(sentenceHumanDatabase.shape[0]):\n",
    "        output[i] /= (np.linalg.norm(sentenceHumanDatabase[i], ord=1))*inputAb\n",
    "    sumAll = np.sum(output)\n",
    "    output = output/sumAll\n",
    "    outIdx = np.argmax(output)\n",
    "    return idx2sen[outIdx+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'senHumanVec.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-aa6dd633f96c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-16-aa6dd633f96c>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0msentenceDatabase\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx2sen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msen2vec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprepare_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0msentenceHumanDatabase\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx2senHuman\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msenHuman2vec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprepare_model_human\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mtestIdx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midx2sen\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtestIdx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-963b4d70b898>\u001b[0m in \u001b[0;36mprepare_model_human\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0msenHuman2vec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentenceHumanVectorFile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m                 \u001b[0msentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'senHumanVec.txt'"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    sentenceDatabase, idx2sen, sen2vec = prepare_model()\n",
    "    sentenceHumanDatabase, idx2senHuman, senHuman2vec = prepare_model_human()\n",
    "    testIdx = 300\n",
    "    print(idx2sen[testIdx])\n",
    "    print(talkHmanVec(sentenceDatabase, idx2sen, sen2vec, idx2sen, np.array(sen2vec[idx2sen[testIdx]])))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
