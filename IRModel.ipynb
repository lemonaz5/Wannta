{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing IR Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup commonly used function and constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "def save_object(obj, filename):\n",
    "    with open(filename, 'wb') as output:\n",
    "        pickle.dump(obj, output, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_object(filename):\n",
    "    r = {}\n",
    "    with open(filename, 'rb') as f:\n",
    "        r = pickle.load(f)\n",
    "    return r\n",
    "\n",
    "sentenceVectorFile = 'senVec.txt'\n",
    "sentenceDictFile = 'sen2vec.pkl'\n",
    "tokenizeFile = 'tokenized_out.txt'\n",
    "sentencesFile = 'sentences.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getSentenceVector(file = None, text = None, Format = True):\n",
    "    args = [\"/data2/fasttext/fasttext\", \"print-sentence-vectors\", \"/data2/cc.th.300.bin\"]\n",
    "    \n",
    "    if(text != None):\n",
    "        popen = subprocess.Popen(args,stdin=subprocess.PIPE, stdout=subprocess.PIPE)\n",
    "        output = popen.communicate(text.encode())[0]\n",
    "        popen.kill()\n",
    "        return np.array([line.split(' ')[:-1] for line in output.decode('utf8').split('\\n')[:-1]], dtype = np.float)\n",
    "    \n",
    "    elif(file != None):\n",
    "        f = open(file)\n",
    "        o = open(sentenceVectorFile, 'w')\n",
    "        popen = subprocess.Popen(args,stdin=f, stdout=o)   \n",
    "        popen.wait()\n",
    "#         output = popen.stdout.read()\n",
    "        f.close()\n",
    "        o.close()\n",
    "        popen.kill()\n",
    "#         if(Format):\n",
    "#             return np.array([line.split(' ')[:-1] for line in output.decode('utf8').split('\\n')[:-1]])\n",
    "#         else:\n",
    "#             return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pythainlp.tokenize import word_tokenize\n",
    "\n",
    "def sentenceTokenize(inputSentence):\n",
    "    # Tokenize\n",
    "    tokenized = word_tokenize(inputSentence)\n",
    "    newTokenize = []\n",
    "    for w in tokenized:\n",
    "        newTokenize += word_tokenize(w, engine='newmm')\n",
    "    return \" \".join(newTokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_memory():\n",
    "    sentences = []\n",
    "    with open(tokenizeFile, 'r') as fp:\n",
    "        for idx, line in enumerate(fp):\n",
    "            sentences.append(\" \".join(line.strip().split('|')))\n",
    "\n",
    "    with open(sentencesFile, 'w') as fp:\n",
    "        for idx, sen in enumerate(sentences):\n",
    "            if idx%2 != 0:\n",
    "                fp.write(\"{}\\n\".format(sen))\n",
    "  \n",
    "    getSentenceVector(file = sentencesFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prepare_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define prepare_model function\n",
    "By checking if file at 'sentenceDictFile' variable exist or not. If not it will create such file, otherwise it will load from file. The output of this function is sentenceDatabase which is a numpy array with dimension of length of all sentence in database by 300, each row is a sentence vector from precompiled fastText. Next is sen2vec is like sentenceDatabase but instead of index of number, the index is the sentence itself. And lastly idx2sen which connected the gap between the two previous mentioned variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_model():\n",
    "    sentencesTokenized = []\n",
    "    with open(tokenizeFile, 'r') as file:\n",
    "        for line in file:\n",
    "            sentencesTokenized.append(\"\".join(line.strip().split(\"|\")))\n",
    "    \n",
    "    if os.path.isfile(sentenceDictFile):\n",
    "        sen2vec = load_object(sentenceDictFile)\n",
    "        idx2sen = {}\n",
    "        for idx, sen in enumerate(sen2vec):\n",
    "            idx2sen[idx] = sen\n",
    "        \n",
    "        sentenceDatabase = np.zeros((len(sen2vec), 300))\n",
    "        for i in range(len(sen2vec)):\n",
    "            for j in range(300):\n",
    "                sentenceDatabase[i][j] = sen2vec[idx2sen[i]][j]\n",
    "        return sentenceDatabase, idx2sen, sen2vec\n",
    "    else:\n",
    "        sen2vec = {}\n",
    "        with open(sentenceVectorFile, 'r') as file:\n",
    "            for idx, line in enumerate(file):\n",
    "                vector = line.strip().split(' ')[-300:]\n",
    "                sentence = sentencesTokenized[idx]\n",
    "                sen2vec[sentence] = list(map(lambda x: float(x), vector))\n",
    "        save_object(sen2vec, sentenceDictFile)\n",
    "        idx2sen = {}\n",
    "        for idx, sen in enumerate(sen2vec):\n",
    "            idx2sen[idx] = sen\n",
    "        \n",
    "        sentenceDatabase = np.zeros((len(sen2vec), 300))\n",
    "        for i in range(len(sen2vec)):\n",
    "            for j in range(300):\n",
    "                sentenceDatabase[i][j] = sen2vec[idx2sen[i]][j]\n",
    "        return sentenceDatabase, idx2sen, sen2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define talkVec function\n",
    "This function compare inputSentenceVector (expected to be np array with dimension of (300, )) with rest of sentenceDatabase using consine similarity, and output the cloest sentence in database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def talkVec(sentenceDatabase, idx2sen, sen2vec, inputSentenceVector):\n",
    "    inputAb = np.linalg.norm(inputSentenceVector,ord=1)\n",
    "    output = sentenceDatabase.dot(inputSentenceVector)\n",
    "    for i in range(sentenceDatabase.shape[0]):\n",
    "        output[i] /= (np.linalg.norm(sentenceDatabase[i], ord=1))*inputAb\n",
    "    sumAll = np.sum(output)\n",
    "    output = output/sumAll\n",
    "    outIdx = np.argmax(output)\n",
    "    return idx2sen[outIdx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define main function\n",
    "As of right now. this is for testing only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizing...\n",
      "gen vector...\n",
      "ทำงาน   database   ยัง\n",
      "เออฟ่ะ 555 มปรนะ คิดซะว่าเค้าพลาดละ เอาจริง ไปละมี2ตำแหน่ง Operation/marketing\n",
      "CPU times: user 253 ms, sys: 69.6 ms, total: 323 ms\n",
      "Wall time: 22 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def main():\n",
    "    sentenceDatabase, idx2sen, sen2vec = prepare_model()\n",
    "    while(True):\n",
    "        print('>')\n",
    "#     text = \"ทำงาน database ยัง\"\n",
    "        text = input()\n",
    "        print('tokenizing...')\n",
    "        text = sentenceTokenize(text)\n",
    "        print('gen vector...')\n",
    "        sent_vec = getSentenceVector(text = text)[0]\n",
    "        print(text)\n",
    "        print(talkVec(sentenceDatabase, idx2sen, sen2vec, sent_vec))\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing another IR Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentenceHumanVectorFile = 'senHumanVec.txt'\n",
    "sentenceHumanDictFile = 'senHuman2vec.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_model_human():\n",
    "    if os.path.isfile(sentenceHumanDictFile):\n",
    "        senHuman2vec = load_object(sentenceHumanDictFile)\n",
    "        idx2senHuman = {}\n",
    "        for idx, sen in enumerate(senHuman2vec):\n",
    "            idx2senHuman[idx] = sen\n",
    "        \n",
    "        sentenceHumanDatabase = np.zeros((len(senHuman2vec), 300))\n",
    "        for i in range(len(senHuman2vec)):\n",
    "            for j in range(300):\n",
    "                sentenceHumanDatabase[i][j] = senHuman2vec[idx2senHuman[i]][j]\n",
    "        return sentenceHumanDatabase, idx2senHuman, senHuman2vec\n",
    "    else:\n",
    "        senHuman2vec = {}\n",
    "        with open(sentenceHumanVectorFile, 'r') as file:\n",
    "            for idx, line in enumerate(file):\n",
    "                sentence = line.strip().split(' ')\n",
    "                vector = sentence[-300:]\n",
    "                sentence = \"\".join(sentence[:len(sentence) - 300])\n",
    "                senHuman2vec[sentence] = list(map(lambda x: float(x), vector))\n",
    "        save_object(senHuman2vec, sentenceHumanDictFile)\n",
    "        \n",
    "        idx2senHuman = {}\n",
    "        for idx, sen in enumerate(senHuman2vec):\n",
    "            idx2senHuman[idx] = sen\n",
    "        \n",
    "        sentenceHumanDatabase = np.zeros((len(senHuman2vec), 300))\n",
    "        for i in range(len(senHuman2vec)):\n",
    "            for j in range(300):\n",
    "                sentenceHumanDatabase[i][j] = senHuman2vec[idx2senHuman[i]][j]\n",
    "        return sentenceHumanDatabase, idx2senHuman, senHuman2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def talkHmanVec(sentenceHumanDatabase, idx2senHuman, senHuman2vec, idx2sen, inputSentenceVector):\n",
    "    inputAb = np.linalg.norm(inputSentenceVector,ord=1)\n",
    "    output = sentenceHumanDatabase.dot(inputSentenceVector)\n",
    "    for i in range(sentenceHumanDatabase.shape[0]):\n",
    "        output[i] /= (np.linalg.norm(sentenceHumanDatabase[i], ord=1))*inputAb\n",
    "    sumAll = np.sum(output)\n",
    "    output = output/sumAll\n",
    "    outIdx = np.argmax(output)\n",
    "    return idx2sen[outIdx+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ละทำmodalแก้หรือเป็นอีกหน้าดีอ่ะ\n",
      "อ้ออาจจะมีไอค่อนด้วยงั้นอีกหน้าก็ดี\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    sentenceDatabase, idx2sen, sen2vec = prepare_model()\n",
    "    sentenceHumanDatabase, idx2senHuman, senHuman2vec = prepare_model_human()\n",
    "    testIdx = 300\n",
    "    print(idx2sen[testIdx])\n",
    "    print(talkHmanVec(sentenceDatabase, idx2sen, sen2vec, idx2sen, np.array(sen2vec[idx2sen[testIdx]])))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
