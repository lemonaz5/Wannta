{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import glob\n",
    "import re\n",
    "import random\n",
    "import collections\n",
    "import os\n",
    "import sys\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "botname = 'แว่นเป๋อ❄️'\n",
    "in_folder = '/home/aroundy/Documents/Chula/3/NLP/stanford-tensorflow-tutorials/assignments/chatbot/Wannta/Linechat/'\n",
    "out_folder = '/'.join(in_folder.split('/')[:-2])+'/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LINE] Chat with nnn..txt\n",
      "[LINE] Chat with Minato Namikaze.txt\n",
      "[LINE] Chat with воѕѕ.txt\n",
      "[LINE] Chat with Jaja _).txt\n",
      "[LINE] Chat with evesuju☆이프슈주.txt\n",
      "[LINE] Chat with findingfon.txt\n",
      "[LINE] Chat with waii..txt\n",
      "[LINE] Chat with Intaniger.txt\n",
      "[LINE] Chat with Jame.txt\n",
      "[LINE] Chat with Tong.txt\n",
      "[LINE] Chat with Fair 1.txt\n",
      "[LINE] Chat with Ter.txt\n",
      "[LINE] Chat with OaT.txt\n",
      "[LINE] Chat with num.txt\n",
      "[LINE] Chat with Nice.txt\n",
      "[LINE] Chat with Tam♡.txt\n",
      "[LINE] Chat with K.txt\n",
      "[LINE] Chat with .เจี๊ยบ..txt\n",
      "[LINE] Chat with Tan.txt\n",
      "[LINE] Chat with Thipok123.txt\n",
      "[LINE] Chat with momo.txt\n",
      "[LINE] Chat with -Gun.txt\n",
      "[LINE] Chat with Brighttiies 明.txt\n",
      "[LINE] Chat with .ŇʋИә ♡.txt\n",
      "[LINE] Chat with TheBrave Yongyee.txt\n",
      "[LINE] Chat with Progorn.txt\n",
      "[LINE] Chat with papoy.txt\n",
      "[LINE] Chat with Fair.txt\n",
      "[LINE] Chat with cp..txt\n",
      "[LINE] Chat with Proud'.txt\n",
      "[LINE] Chat with Warm.txt\n",
      "[LINE] Chat with petchphi.txt\n",
      "[LINE] Chat with Napatt.txt\n",
      "[LINE] Chat with pibi.txt\n",
      "[LINE] Chat with Noey.txt\n",
      "[LINE] Chat with sukree.txt\n",
      "[LINE] Chat with Meandae.txt\n",
      "[LINE] Chat with Minkynd.♡.txt\n",
      "[LINE] Chat with Poom.txt\n",
      "[LINE] Chat with Praewa.txt\n",
      "[LINE] Chat with Game11189.txt\n",
      "[LINE] Chat with Mal2u.txt\n",
      "[LINE] Chat with ball.txt\n",
      "[LINE] Chat with FM.txt\n",
      "[LINE] Chat with mos.txt\n",
      "[LINE] Chat with M@!.txt\n",
      "[LINE] Chat with Tun Max.txt\n",
      "[LINE] Chat with bbeauu.txt\n",
      "[LINE] Chat with LAAMSANG.txt\n",
      "[LINE] Chat with b_bom_by 1.txt\n",
      "[LINE] Chat with Peam.txt\n",
      "[LINE] Chat with โบน.txt\n",
      "[LINE] Chat with KNT..txt\n",
      "[LINE] Chat with Mick.txt\n",
      "[LINE] Chat with Miharuちゃん.txt\n",
      "[LINE] Chat with b_bom_by.txt\n",
      "[LINE] Chat with TG.txt\n",
      "[LINE] Chat with .LNIW.txt\n",
      "[LINE] Chat with FAIPHAD.txt\n",
      "[LINE] Chat with NoTT'z.txt\n",
      "[LINE] Chat with Ten'ten.txt\n",
      "[LINE] Chat with mew.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import code\n",
    "convs = []\n",
    "for file in os.listdir(in_folder):\n",
    "    if(file in [\"out.txt\",\"Line chat.zip\"]):\n",
    "        continue\n",
    "    with open(in_folder+file, 'r') as f:\n",
    "        print(file)\n",
    "\n",
    "        conv = []\n",
    "        isFirst = True\n",
    "        isMultipleLine = False\n",
    "        is_human_first = True\n",
    "\n",
    "        combined_sent = []\n",
    "        prev_send = ''\n",
    "        \n",
    "        for line in f:            \n",
    "            sender = ''\n",
    "            sent = ''\n",
    "            spl = line.strip().split('\\t')\n",
    "            \n",
    "            if(len(spl) == 3):\n",
    "                sender = spl[1]\n",
    "                sent = spl[2]\n",
    "                \n",
    "\n",
    "                if(sent in ['[Photo]','[Sticker]','[File]' , '[Video]']):\n",
    "                    continue\n",
    "                if(sent[0] == '☎'):\n",
    "                    continue\n",
    "                \n",
    "                if(sent[0] == '\"'):\n",
    "                    sent = sent[1:]\n",
    "                    isMultipleLine = True\n",
    "                    \n",
    "                if(sender != prev_send):\n",
    "                    conv.append(prev_send + \"\\t\" +' '.join(combined_sent))\n",
    "                    combined_sent = [sent]\n",
    "                    prev_send = sender\n",
    "                    \n",
    "                else:\n",
    "                    combined_sent.append(sent)\n",
    "            \n",
    "            elif(isMultipleLine):\n",
    "                if(len(line) > 1 and line[-2] == '\"'):\n",
    "                    isMultipleLine = False\n",
    "                combined_sent.append(line.strip().strip('\"'))\n",
    "                \n",
    "\n",
    "#         combined_sent.append(sent)\n",
    "        if(prev_send == botname):\n",
    "            conv.append(prev_send + '\\t' +' '.join(combined_sent))\n",
    "        \n",
    "        cidx=1\n",
    "        while(conv[cidx].split('\\t')[0] == botname):\n",
    "            cidx+=1\n",
    "        \n",
    "        convs.append(conv[cidx:])\n",
    "        \n",
    "with open(out_folder+'debug-sentences-1.txt', 'w') as o:\n",
    "    for con in convs:\n",
    "        for sent in con:\n",
    "            o.write(sent + '\\n')\n",
    "            \n",
    "with open(out_folder+'sentences-1.txt', 'w') as o:\n",
    "    for con in convs:\n",
    "        for sent in con:\n",
    "            o.write(sent.split('\\t')[1] + '\\n')            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conver = []\n",
    "with open(out_path+'sentences.txt') as f:\n",
    "    for line in f:\n",
    "        conver.append(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tokenize\n",
    "- This tokenize used Deepcut.\n",
    "- May use TokenizeWords.ipynb which used Pythainlp instead. \n",
    "  If so, ignore this topic and run that file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/aroundy/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['สวัสดี', 'เพื่อน', 'ๆ']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import deepcut\n",
    "deepcut.tokenize(\"สวัสดีเพื่อนๆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "token = []\n",
    "for sent in conver:\n",
    "    token.append(deepcut.tokenize(sent.strip()))\n",
    "print(token[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token_set = set()\n",
    "for words in token:\n",
    "    token_set = token_set.union(words)\n",
    "\n",
    "token_list = list(token_set)\n",
    "print(token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#fromm Tipok code style\n",
    "token = []\n",
    "with open('tokenized_out.txt','r') as f:\n",
    "    for line in f:\n",
    "        token.append(line.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dictionary\n",
    "- If you want sentense to have a start/stop word, Comment out thw two line before return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_dataset_dict(input_list):\n",
    "    words = []\n",
    "    for review in input_list:\n",
    "        for word in review:\n",
    "            words.append(word)\n",
    "    word_count = list()\n",
    "    #use set and len to get the number of unique words\n",
    "    word_count.extend(collections.Counter(words).most_common(len(set(words))))\n",
    "    \n",
    "    #threshold = len(input_text)//1e6\n",
    "    threshold = 1\n",
    "    print(\"Threshold\", threshold)\n",
    "    \n",
    "    word_unk = set()\n",
    "    num_unk = 0\n",
    "    idx = len(word_count)-1\n",
    "    while word_count[idx][1] <= threshold:\n",
    "        word_unk.add(word_count[idx][0])\n",
    "        num_unk += word_count[idx][1]\n",
    "        idx -= 1\n",
    "\n",
    "    word_count = word_count[:idx+1]\n",
    "    \n",
    "    #include a token for unknown word\n",
    "    word_count.append((\"เหรอ\",num_unk))\n",
    "    word_count = sorted(word_count, key=lambda x: -x[1])\n",
    "    #word_count = [x for x in word_count if x[0] not in word_unk]\n",
    "    \n",
    "    #print out 10 most frequent words\n",
    "    print(word_count[:10])\n",
    "    print(\"#unk\" ,num_unk)\n",
    "    \n",
    "    dictionary = dict()\n",
    "    dictionary[\"for_keras_zero_padding\"] = 0\n",
    "    for word in word_count:\n",
    "        dictionary[word[0]] = len(dictionary)\n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    datas = list()\n",
    "    for review in input_list:\n",
    "        data = []\n",
    "        for word in review:\n",
    "            if word in word_unk:\n",
    "                data.append(dictionary[\"เหรอ\"])\n",
    "            else:\n",
    "                data.append(dictionary[word])\n",
    "        datas.append(data)\n",
    "#     dictionary[\"<s>\"] = len(dictionary)\n",
    "#     dictionary[\"</s>\"] = len(dictionary)\n",
    "\n",
    "    return datas,dictionary, reverse_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold 1\n",
      "[('555', 13511), ('ไป', 7497), ('ไม่เข้าใจอ่ะ พูดอย่างอื่นได้ป่าว', 7490), ('เค้า', 5302), ('อ่ะ', 4971), ('แก', 4600), ('ไม่', 4438), ('ก็', 4437), ('มัน', 4099), ('จะ', 4072)]\n",
      "#unk 7490\n"
     ]
    }
   ],
   "source": [
    "dataset,dictionary, reverse_dictionary=create_dataset_dict(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len data set: 35434\n",
      "len dictionary: 8603\n",
      "output sample (dataset): [[105, 85, 43, 269], [867, 1276, 104, 1260], [307, 36, 7, 47, 112, 81], [6, 178, 112, 207, 1409, 930, 905, 173, 31, 1, 1469, 100, 35, 970, 590, 64, 6, 2, 142, 218, 57, 173, 181, 23, 2, 919, 23, 1155, 17, 1], [2452, 21, 29, 112, 662, 2, 2216, 17, 178, 1059, 905, 5131, 1940, 540, 3523], [112, 82, 433, 199, 7, 2996, 42, 9, 358, 8, 191, 152], [2606, 1803, 434, 1, 102, 1029, 204, 83, 57, 2, 2216, 17, 34, 1029, 6295, 516, 770], [8, 2, 2216, 12, 1640, 199, 375, 202, 7, 43, 1803, 8, 561, 310], [722, 250, 545, 28, 1015, 752, 2217, 27], [43, 498, 141, 499, 291, 655, 1]]\n",
      "output sample (dictionary): {'for_keras_zero_padding': 0, '555': 1, 'ไป': 2, 'ไม่เข้าใจอ่ะ พูดอย่างอื่นได้ป่าว': 3, 'เค้า': 4, 'อ่ะ': 5, 'แก': 6, 'ไม่': 7, 'ก็': 8, 'มัน': 9} \n",
      "dictionary size:  8603\n",
      "output sample (reverse dictionary): {0: 'for_keras_zero_padding', 1: '555', 2: 'ไป', 3: 'ไม่เข้าใจอ่ะ พูดอย่างอื่นได้ป่าว', 4: 'เค้า', 5: 'อ่ะ', 6: 'แก', 7: 'ไม่', 8: 'ก็', 9: 'มัน'}\n"
     ]
    }
   ],
   "source": [
    "print(\"len data set:\", len(dataset))\n",
    "print(\"len dictionary:\", len(dictionary))\n",
    "print(\"output sample (dataset):\",dataset[:10])\n",
    "print(\"output sample (dictionary):\",{k: dictionary[k] for k in list(dictionary)[:10]}, \"\\ndictionary size: \",len(dictionary))\n",
    "print(\"output sample (reverse dictionary):\",{k: reverse_dictionary[k] for k in list(reverse_dictionary)[:10]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out_path = '/home/aroundy/Documents/Chula/3/NLP/stanford-tensorflow-tutorials/assignments/chatbot/Wannta/prepro/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- run this one if you want a start/stop word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "human = []\n",
    "bot = []\n",
    "even = True\n",
    "for line in dataset:\n",
    "    if (even):\n",
    "        human.append(line)\n",
    "        even = False\n",
    "    else:\n",
    "        bot.append(line)\n",
    "        even = True\n",
    "test_train_split = 0.8\n",
    "        \n",
    "human_train = human[:int(len(human)*test_train_split)]\n",
    "human_test = human[int(len(human)*test_train_split):]\n",
    "bot_train = bot[:int(len(bot)*test_train_split)]\n",
    "bot_test = bot[int(len(bot)*test_train_split):]\n",
    "\n",
    "start = str(dictionary[\"<s>\"])\n",
    "end = str(dictionary[\"</s>\"])\n",
    "\n",
    "with open(out_path+'human_train.txt', 'w') as o:\n",
    "    for line in human_train:\n",
    "        line = \" \".join([str(x) for x in line])\n",
    "        o.write(line+'\\n')\n",
    "with open(out_path+'bot_train.txt', 'w') as o:\n",
    "    for line in bot_train:\n",
    "        line = \" \".join([start]+[str(x) for x in line]+[end])\n",
    "        o.write(line+'\\n')\n",
    "with open(out_path+'human_test.txt', 'w') as o:\n",
    "    for line in human_test:\n",
    "        line = \" \".join([str(x) for x in line])\n",
    "        o.write(line+'\\n')\n",
    "with open(out_path+'bot_test.txt', 'w') as o:\n",
    "    for line in bot_test:\n",
    "        line = \" \".join([start]+[str(x) for x in line]+[end])\n",
    "        o.write(line+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dict_list = []\n",
    "with open(out_path+'vocab.txt','w') as o:\n",
    "    for value in dictionary:\n",
    "        o.write(value+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Otherwise, run this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "human = []\n",
    "bot = []\n",
    "even = True\n",
    "for line in dataset:\n",
    "    if (even):\n",
    "        human.append(line)\n",
    "        even = False\n",
    "    else:\n",
    "        bot.append(line)\n",
    "        even = True\n",
    "test_train_split = 0.8\n",
    "        \n",
    "human_train = human[:int(len(human)*test_train_split)]\n",
    "human_test = human[int(len(human)*test_train_split):]\n",
    "bot_train = bot[:int(len(bot)*test_train_split)]\n",
    "bot_test = bot[int(len(bot)*test_train_split):]\n",
    "\n",
    "# start = str(dictionary[\"<s>\"])\n",
    "# end = str(dictionary[\"</s>\"])\n",
    "\n",
    "with open(out_path+'human_train.txt', 'w') as o:\n",
    "    for line in human_train:\n",
    "        line = \" \".join([str(x) for x in line])\n",
    "        o.write(line+'\\n')\n",
    "with open(out_path+'bot_train_nostart.txt', 'w') as o:\n",
    "    for line in bot_train:\n",
    "        line = \" \".join([str(x) for x in line])\n",
    "        o.write(line+'\\n')\n",
    "with open(out_path+'human_test.txt', 'w') as o:\n",
    "    for line in human_test:\n",
    "        line = \" \".join([str(x) for x in line])\n",
    "        o.write(line+'\\n')\n",
    "with open(out_path+'bot_test_nostart.txt', 'w') as o:\n",
    "    for line in bot_test:\n",
    "        line = \" \".join([str(x) for x in line])\n",
    "        o.write(line+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dict_list = []\n",
    "with open(out_path+'vocab_nostart.txt','w') as o:\n",
    "    for value in dictionary:\n",
    "        o.write(value+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3515"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary[\"UNK\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "ready for training in chat.py "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
