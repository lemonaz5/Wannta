{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import glob\n",
    "import re\n",
    "import random\n",
    "import collections\n",
    "import os\n",
    "import sys\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "botname = 'à¹à¸§à¹ˆà¸™à¹€à¸›à¹‹à¸­â„ï¸'\n",
    "in_folder = '/data2/backup/'\n",
    "out_folder = '/'.join(in_folder.split('/')[:-2])+'/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LINE] Chat with mos.txt\n",
      "[LINE] Chat with Fair.txt\n",
      "[LINE] Chat with KNT.txt\n",
      "[LINE] Chat with NoTT'z ðŸ”©.txt\n",
      "LINE Chat with Jaja 2559 (1).txt\n",
      "[LINE] Chat with OaT.txt\n",
      "[LINE] Chat with Ten'ten.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "convs = []\n",
    "for file in os.listdir(in_folder):\n",
    "    if(file in [\"out.txt\"]):\n",
    "        continue\n",
    "    with open(in_folder+file) as f:\n",
    "        print(file)\n",
    "\n",
    "        conv = []\n",
    "        isFirst = True\n",
    "        isMultipleLine = False\n",
    "        is_human_first = True\n",
    "\n",
    "        combined_sent = []\n",
    "        prev_send = ''\n",
    "        \n",
    "        for line in f:            \n",
    "            sender = ''\n",
    "            sent = ''\n",
    "            spl = line.strip().split('\\t')\n",
    "            \n",
    "            if(len(spl) == 3):\n",
    "                sender = spl[1]\n",
    "                sent = spl[2]\n",
    "                \n",
    "                if(isFirst):\n",
    "                    if(sender == botname):\n",
    "                        is_human_first = False\n",
    "                    isFirst = False\n",
    "\n",
    "                if(sent in ['[Photo]','[Sticker]','[File]' , '[Video]']):\n",
    "                    continue\n",
    "                if(sent[0] == 'â˜Ž'):\n",
    "                    continue\n",
    "                \n",
    "                if(sent[0] == '\"'):\n",
    "                    sent = sent[1:]\n",
    "                    isMultipleLine = True\n",
    "                    \n",
    "                if(sender != prev_send):\n",
    "                    conv.append(prev_send + \"\\t\" +' '.join(combined_sent))\n",
    "                    combined_sent = [sent]\n",
    "                    prev_send = sender\n",
    "                    \n",
    "                else:\n",
    "                    combined_sent.append(sent)\n",
    "            \n",
    "            elif(isMultipleLine):\n",
    "                if(len(line) > 1 and line[-2] == '\"'):\n",
    "                    isMultipleLine = False\n",
    "                combined_sent.append(line.strip().strip('\"'))\n",
    "                \n",
    "\n",
    "#         combined_sent.append(sent)\n",
    "        if(prev_send != botname):\n",
    "            conv.append(prev_send + '\\t' +' '.join(combined_sent))\n",
    "        \n",
    "        \n",
    "        if( is_human_first ):\n",
    "            conv = conv[1:]\n",
    "        else:\n",
    "            conv = conv[2:]\n",
    "        \n",
    "        convs.append(conv[:-1])\n",
    "        \n",
    "with open(out_folder+'debug-sentences.txt', 'w') as o:\n",
    "    for con in convs:\n",
    "        for sent in con:\n",
    "            o.write(sent + '\\n')\n",
    "            \n",
    "with open(out_folder+'sentences.txt', 'w') as o:\n",
    "    for con in convs:\n",
    "        for sent in con:\n",
    "            o.write(sent.split('\\t')[1] + '\\n')            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out_path = '/data2/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conver = []\n",
    "with open(out_path+'sentences.txt') as f:\n",
    "    for line in f:\n",
    "        conver.append(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import deepcut\n",
    "deepcut.tokenize(\"à¸ªà¸§à¸±à¸ªà¸”à¸µà¹€à¸žà¸·à¹ˆà¸­à¸™à¹†\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "token = []\n",
    "for sent in conver:\n",
    "    token.append(deepcut.tokenize(sent.strip()))\n",
    "print(token[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "token_set = set()\n",
    "for words in token:\n",
    "    token_set = token_set.union(words)\n",
    "\n",
    "token_list = list(token_set)\n",
    "print(token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_dataset_dict(input_list):\n",
    "    words = []\n",
    "    for review in input_list:\n",
    "        for word in review:\n",
    "            words.append(word)\n",
    "    word_count = list()\n",
    "    #use set and len to get the number of unique words\n",
    "    word_count.extend(collections.Counter(words).most_common(len(set(words))))\n",
    "    \n",
    "    #threshold = len(input_text)//1e6\n",
    "    threshold = 3\n",
    "    print(\"Threshold\", threshold)\n",
    "    \n",
    "    word_unk = set()\n",
    "    num_unk = 0\n",
    "    idx = len(word_count)-1\n",
    "    while word_count[idx][1] <= threshold:\n",
    "        word_unk.add(word_count[idx][0])\n",
    "        num_unk += word_count[idx][1]\n",
    "        idx -= 1\n",
    "\n",
    "    word_count = word_count[:idx+1]\n",
    "    \n",
    "    #include a token for unknown word\n",
    "    word_count.append((\"UNK\",num_unk))\n",
    "    word_count = sorted(word_count, key=lambda x: -x[1])\n",
    "    #word_count = [x for x in word_count if x[0] not in word_unk]\n",
    "    \n",
    "    #print out 10 most frequent words\n",
    "    print(word_count[:10])\n",
    "    print(\"#unk\" ,num_unk)\n",
    "    \n",
    "    dictionary = dict()\n",
    "    dictionary[\"for_keras_zero_padding\"] = 0\n",
    "    for word in word_count:\n",
    "        dictionary[word[0]] = len(dictionary)\n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    datas = list()\n",
    "    for review in input_list:\n",
    "        data = []\n",
    "        for word in review:\n",
    "            if word in word_unk:\n",
    "                data.append(dictionary[\"UNK\"])\n",
    "            else:\n",
    "                data.append(dictionary[word])\n",
    "        datas.append(data)\n",
    "\n",
    "    return datas,dictionary, reverse_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset,dictionary, reverse_dictionary=create_dataset_dict(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"len data set:\", len(dataset))\n",
    "print(\"len dictionary:\", len(dictionary))\n",
    "print(\"output sample (dataset):\",dataset[:10])\n",
    "print(\"output sample (dictionary):\",{k: dictionary[k] for k in list(dictionary)[:10]}, \"\\ndictionary size: \",len(dictionary))\n",
    "print(\"output sample (reverse dictionary):\",{k: reverse_dictionary[k] for k in list(reverse_dictionary)[:10]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "human = []\n",
    "bot = []\n",
    "even = True\n",
    "for line in dataset:\n",
    "    if (even):\n",
    "        human.append(line)\n",
    "        even = False\n",
    "    else:\n",
    "        bot.append(line)\n",
    "        even = True\n",
    "test_train_split = 0.8\n",
    "        \n",
    "human_train = human[:int(len(human)*test_train_split)]\n",
    "human_test = human[int(len(human)*test_train_split):]\n",
    "bot_train = bot[:int(len(bot)*test_train_split)]\n",
    "bot_test = bot[int(len(bot)*test_train_split):]\n",
    "\n",
    "with open(out_path+'human_train.txt', 'w') as o:\n",
    "    for line in human_train:\n",
    "        line = \" \".join([str(x) for x in line])\n",
    "        o.write(line+'\\n')\n",
    "with open(out_path+'bot_train.txt', 'w') as o:\n",
    "    for line in bot_train:\n",
    "        line = \" \".join([str(x) for x in line])\n",
    "        o.write(line+'\\n')\n",
    "with open(out_path+'human_test.txt', 'w') as o:\n",
    "    for line in human_test:\n",
    "        line = \" \".join([str(x) for x in line])\n",
    "        o.write(line+'\\n')\n",
    "with open(out_path+'bot_test.txt', 'w') as o:\n",
    "    for line in bot_test:\n",
    "        line = \" \".join([str(x) for x in line])\n",
    "        o.write(line+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dict_list = []\n",
    "with open(out_path+'vocab.txt','w') as o:\n",
    "    for value in dictionary:\n",
    "        o.write(value+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
